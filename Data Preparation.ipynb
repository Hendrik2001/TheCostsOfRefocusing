{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "MAKEHF1DATA"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ac9823ff687378d"
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-07T16:52:51.420245900Z",
     "start_time": "2024-08-07T16:52:12.147813200Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load the merged data\n",
    "merged_df = pd.read_csv('merged_df.csv', low_memory=False)\n",
    "\n",
    "# Load the mydate converter data\n",
    "mydate_converter = pd.read_csv('mydate_converter.csv', names=['year', 'month', 'mydate'], skiprows=[0])\n",
    "\n",
    "# Remove non-numeric rows from mydate_converter\n",
    "mydate_converter = mydate_converter[pd.to_numeric(mydate_converter['year'], errors='coerce').notnull()]\n",
    "mydate_converter['year'] = mydate_converter['year'].astype(int)\n",
    "mydate_converter['month'] = mydate_converter['month'].astype(int)\n",
    "mydate_converter['mydate'] = mydate_converter['mydate'].astype(int)\n",
    "\n",
    "# Extract year and month from the date in merged_df\n",
    "merged_df['year'] = pd.to_numeric(merged_df['date'].str[:4], errors='coerce')\n",
    "merged_df['month'] = pd.to_numeric(merged_df['date'].str[5:7], errors='coerce')\n",
    "\n",
    "# Ensure the year and month columns are of the same data type\n",
    "merged_df['year'] = merged_df['year'].astype(int)\n",
    "merged_df['month'] = merged_df['month'].astype(int)\n",
    "\n",
    "# Merge with mydate converter\n",
    "merged_df = merged_df.merge(mydate_converter, on=['year', 'month'], how='left')\n",
    "\n",
    "# Step 2: Filter and Clean the Data\n",
    "# Keep records from 1994 onwards\n",
    "merged_df = merged_df[merged_df['year'] >= 1994]\n",
    "\n",
    "# Generate elapsed time\n",
    "merged_df['maxmydate'] = pd.to_numeric(merged_df.groupby('id')['mydate'].transform('max'), errors='coerce')\n",
    "merged_df['minmydate'] = pd.to_numeric(merged_df.groupby('id')['mydate'].transform('min'), errors='coerce')\n",
    "merged_df['elapsedtime'] = merged_df['maxmydate'] - merged_df['minmydate'] + 1\n",
    "\n",
    "# Filter out invalid records\n",
    "merged_df = merged_df[(merged_df['aum'] <= 100000000000) & (merged_df['aum'] >= 1000000)]\n",
    "merged_df = merged_df[merged_df['ret'] <= 1000]\n",
    "\n",
    "# Identify sporadic reporters\n",
    "retcounter = merged_df.groupby('id').size().reset_index(name='ret_counter')\n",
    "\n",
    "merged_df = merged_df.merge(retcounter, on='id', how='left')\n",
    "merged_df['sporadic_dum'] = (merged_df['elapsedtime'] > merged_df['ret_counter']).astype(int)\n",
    "max_sporadic = merged_df.groupby('id')['sporadic_dum'].transform('max')\n",
    "merged_df = merged_df[max_sporadic == 0]\n",
    "\n",
    "# Drop funds with fewer than 12 months of data\n",
    "merged_df = merged_df[merged_df['ret_counter'] >= 12]\n",
    "merged_df = merged_df[merged_df['ret'].notna()]\n",
    "\n",
    "# Drop unnecessary columns\n",
    "merged_df.drop(columns=['elapsedtime', 'ret_counter'], inplace=True)\n",
    "\n",
    "# Save the cleaned data\n",
    "merged_df.to_csv('tass2.csv', index=False)\n",
    "\n",
    "# Step 3: Break Data into Pre-Crisis, Crisis, and Post-Crisis Periods\n",
    "# Split data into pre-crisis, crisis, and post-crisis periods\n",
    "tass_pre = merged_df[merged_df['mydate'] < 574]\n",
    "tass_pre.to_csv('tass2_pre.csv', index=False)\n",
    "\n",
    "tass_crisis = merged_df[(merged_df['mydate'] >= 574) & (merged_df['mydate'] <= 593)]\n",
    "tass_crisis.to_csv('tass2_crisis.csv', index=False)\n",
    "\n",
    "tass_post = merged_df[merged_df['mydate'] > 593]\n",
    "tass_post.to_csv('tass2_post.csv', index=False)\n",
    "\n",
    "# Step 4: Perform AR1 Adjustment\n",
    "def ar1_adjustment(df, period_name):\n",
    "    df = df.sort_values(by=['id', 'mydate'])\n",
    "    df['rho'] = 0.0\n",
    "    unique_ids = df['id'].unique()\n",
    "\n",
    "    for unique_id in unique_ids:\n",
    "        sub_df = df[df['id'] == unique_id]\n",
    "        if len(sub_df) > 1:\n",
    "            model = sm.OLS(sub_df['ret'].iloc[1:], sm.add_constant(sub_df['ret'].shift(1).iloc[1:])).fit()\n",
    "            rho = model.params.iloc[1] if len(model.params) > 1 else 0  # Use iloc for positional access\n",
    "            df.loc[df['id'] == unique_id, 'rho'] = rho\n",
    "\n",
    "    df['ret_star'] = (df['ret'] - df['rho'] * df['ret'].shift(1)) / (1 - df['rho'])\n",
    "    df.to_csv(f'tass4_{period_name}.csv', index=False)\n",
    "    return df\n",
    "\n",
    "# AR1 adjustment for pre-crisis period\n",
    "tass_pre_adjusted = ar1_adjustment(tass_pre, 'pre')\n",
    "\n",
    "# AR1 adjustment for crisis period\n",
    "tass_crisis_adjusted = ar1_adjustment(tass_crisis, 'crisis')\n",
    "\n",
    "# AR1 adjustment for post-crisis period\n",
    "tass_post_adjusted = ar1_adjustment(tass_post, 'post')\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year  month     yield  mydate\n",
      "0  1993     12  3.513636     407\n",
      "1  1994      1  3.542500     408\n",
      "2  1994      2  3.865789     409\n",
      "3  1994      3  4.319130     410\n",
      "4  1994      4  4.815789     411\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def load_and_clean_data_with_mydate(file_path, mydate_converter, keep_columns=None, drop_na_columns=None):\n",
    "    file_ext = file_path.split('.')[-1]\n",
    "    if file_ext == 'csv':\n",
    "        df = pd.read_csv(file_path)\n",
    "    elif file_ext in ['xls', 'xlsx']:\n",
    "        df = pd.read_excel(file_path)\n",
    "\n",
    "    # Merge with mydate_converter\n",
    "    mydate_converter = mydate_converter[pd.to_numeric(mydate_converter['year'], errors='coerce').notnull()]\n",
    "\n",
    "    mydate_converter['year'] = mydate_converter['year'].astype(int)\n",
    "    mydate_converter['month'] = mydate_converter['month'].astype(int)\n",
    "    mydate_converter['mydate'] = mydate_converter['mydate'].astype(int)\n",
    "\n",
    "    df = df.merge(mydate_converter, on=['year', 'month'], how='left')\n",
    "    \n",
    "    if drop_na_columns:\n",
    "        df = df.dropna(subset=drop_na_columns)\n",
    "    if keep_columns:\n",
    "        df = df[keep_columns]\n",
    "    \n",
    "    df = df.sort_values(by=['year', 'month'])\n",
    "    return df\n",
    "\n",
    "# Load the mydate converter data\n",
    "mydate_converter = pd.read_csv('mydate_converter.csv', names=['year', 'month', 'mydate'], skiprows=[0])\n",
    "\n",
    "# Load factor files with mydate\n",
    "df_ff = load_and_clean_data_with_mydate('Factors/Corrected_FF_Research_Data_Factors.csv', mydate_converter, drop_na_columns=['month'])\n",
    "df_fung_hsieh = load_and_clean_data_with_mydate('Factors/TF-Fac.xlsx', mydate_converter, keep_columns=['PTFSBD', 'PTFSFX', 'PTFSCOM', 'year', 'month'], drop_na_columns=['year'])\n",
    "df_mom = load_and_clean_data_with_mydate('Factors/Corrected_FF_Momentum_Factor.csv', mydate_converter)\n",
    "df_bond = load_and_clean_data_with_mydate('Factors/DBAA_Monthly_Averages.csv', mydate_converter)\n",
    "df_credit = load_and_clean_data_with_mydate('Factors/DGS10_Monthly_Averages.csv', mydate_converter, drop_na_columns=['year'])\n",
    "df_rfr = load_and_clean_data_with_mydate('Factors/interest_rates_monthly.csv', mydate_converter)\n",
    "print(df_rfr.head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T16:52:51.588954400Z",
     "start_time": "2024-08-07T16:52:51.428005100Z"
    }
   },
   "id": "8292d8526a46cf22",
   "execution_count": 126
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to perform AR1 adjustment\n",
    "def ar1_adjustment(df, period_name):\n",
    "    df = df.copy()\n",
    "    df['ret_star'] = df['ret']\n",
    "    unique_ids = df['id'].unique()\n",
    "    for unique_id in unique_ids:\n",
    "        subset = df[df['id'] == unique_id]\n",
    "        if len(subset) > 1:\n",
    "            subset = subset.sort_values(by='mydate')\n",
    "            X = sm.add_constant(subset['ret'].shift(1).dropna())\n",
    "            y = subset['ret'].iloc[1:]\n",
    "            if len(X) == len(y):  # Ensure X and y have the same length\n",
    "                try:\n",
    "                    model = sm.OLS(y, X).fit()\n",
    "                    rho = model.params.iloc[1] if len(model.params) > 1 else 0  # Default to 0 if model fitting fails\n",
    "                    df.loc[df['id'] == unique_id, 'ret_star'] = (df['ret'] - rho * df['ret'].shift(1)) / (1 - rho)\n",
    "                except Exception as e:\n",
    "                    print(f\"Model fitting failed for id {unique_id} with error: {e}\")\n",
    "    df.to_csv(f'tass4_{period_name}.csv', index=False)\n",
    "    return df\n",
    "\n",
    "# Load merged data\n",
    "\n",
    "merged_df['mydate'] = merged_df['mydate'].astype(int)\n",
    "\n",
    "# Define periods\n",
    "pre_crisis_period = merged_df[(merged_df['mydate'] >= 0) & (merged_df['mydate'] < 575)]\n",
    "crisis_period = merged_df[(merged_df['mydate'] >= 575) & (merged_df['mydate'] <= 593)]\n",
    "post_crisis_period = merged_df[(merged_df['mydate'] > 593)]\n",
    "\n",
    "# Perform AR1 adjustment\n",
    "tass4_pre = ar1_adjustment(pre_crisis_period, 'pre')\n",
    "tass4_crisis = ar1_adjustment(crisis_period, 'crisis')\n",
    "tass4_post = ar1_adjustment(post_crisis_period, 'post')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T16:53:50.888483600Z",
     "start_time": "2024-08-07T16:52:51.595009700Z"
    }
   },
   "id": "7c101f2e181229ba",
   "execution_count": 127
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to merge factors with TASS data\n",
    "def merge_factors(df_tass, factors_list):\n",
    "    for factor_df in factors_list:\n",
    "        df_tass = pd.merge(df_tass, factor_df, on=['year', 'month'], how='left', indicator=True, suffixes=('','_remove'))\n",
    "        df_tass.drop([i for i in df_tass.columns if 'remove' in i], axis=1, inplace=True)\n",
    "        df_tass = df_tass[df_tass['_merge'] == 'both'].drop('_merge', axis=1)\n",
    "    return df_tass\n",
    "\n",
    "# Load the adjusted TASS data for each period\n",
    "df_tass4_pre = pd.read_csv('tass4_pre.csv')\n",
    "df_tass4_crisis = pd.read_csv('tass4_crisis.csv')\n",
    "df_tass4_post = pd.read_csv('tass4_post.csv')\n",
    "\n",
    "# List of factor dataframes\n",
    "factors_list = [df_ff, df_fung_hsieh, df_mom, df_bond, df_credit, df_rfr]\n",
    "\n",
    "# Merging factors with TASS data for each period\n",
    "df_tass4_pre = merge_factors(df_tass4_pre, factors_list)\n",
    "df_tass4_crisis = merge_factors(df_tass4_crisis, factors_list)\n",
    "df_tass4_post = merge_factors(df_tass4_post, factors_list)\n",
    "\n",
    "# Save the merged dataframes to CSV for further use\n",
    "df_tass4_pre.to_csv('tass4_pre_merged.csv', index=False)\n",
    "df_tass4_crisis.to_csv('tass4_crisis_merged.csv', index=False)\n",
    "df_tass4_post.to_csv('tass4_post_merged.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T16:53:56.969843600Z",
     "start_time": "2024-08-07T16:53:50.895743400Z"
    }
   },
   "id": "253f360806dc1828",
   "execution_count": 128
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "         id        date       ret        aum  companyid  \\\n0      4075  2006-03-31  0.021399  190388153    18038.0   \n1      4075  2006-05-31  0.007099  219482626    18038.0   \n2      4075  2006-07-31 -0.000299  230121070    18038.0   \n3      4075  2006-09-30  0.001799  233737989    18038.0   \n4      4075  2006-11-30  0.007899  248747495    18038.0   \n...     ...         ...       ...        ...        ...   \n77139  5832  2006-02-28 -0.075599   19079565     1622.0   \n77140  5832  2006-04-30  0.060999   18704055     1622.0   \n77141  5832  2006-06-30 -0.011599   29194946     1622.0   \n77142  5832  2007-08-31  0.003099  135784198     1622.0   \n77143  5832  2007-10-31  0.042499  162374618     1622.0   \n\n               Main Strategy  incentivefee managementfee  year  month  ...  \\\n0      European Long / Short           0.2         0.015  2006      3  ...   \n1      European Long / Short           0.2         0.015  2006      5  ...   \n2      European Long / Short           0.2         0.015  2006      7  ...   \n3      European Long / Short           0.2         0.015  2006      9  ...   \n4      European Long / Short           0.2         0.015  2006     11  ...   \n...                      ...           ...           ...   ...    ...  ...   \n77139       Sector Commodity           NaN           NaN  2006      2  ...   \n77140       Sector Commodity           NaN           NaN  2006      4  ...   \n77141       Sector Commodity           NaN           NaN  2006      6  ...   \n77142       Sector Commodity           NaN           NaN  2007      8  ...   \n77143       Sector Commodity           NaN           NaN  2007     10  ...   \n\n        SMB   HML    RF  PTFSBD  PTFSFX  PTFSCOM   Mom      DBAA     DGS10  \\\n0      3.44  0.60  0.37 -0.0662 -0.1037  -0.0589  1.26  6.411304  4.723913   \n1     -2.96  2.41  0.43 -0.0696  0.1569  -0.0457 -3.70  6.745455  5.110000   \n2     -3.98  2.60  0.40 -0.1733 -0.1157  -0.2304 -2.12  6.762000  5.087500   \n3     -1.36  0.08  0.41 -0.0386 -0.2034   0.0361 -0.96  6.430000  4.719000   \n4      0.70  0.14  0.42 -0.1081  0.4743   0.0003 -1.03  6.203810  4.595238   \n...     ...   ...   ...     ...     ...      ...   ...       ...       ...   \n77139 -0.38 -0.34  0.34 -0.1832 -0.1338  -0.1007 -1.84  6.270000  4.568947   \n77140 -1.42  2.34  0.36  0.1980  0.3177   0.2275  0.64  6.680526  4.990526   \n77141 -0.39  0.85  0.40 -0.1495 -0.1262  -0.0196  1.54  6.779545  5.106364   \n77142 -0.12 -1.86  0.42 -0.0078 -0.0037   0.0807  0.10  6.652174  4.674783   \n77143  0.08 -2.98  0.32 -0.1584  0.0497   0.1178  5.02  6.479545  4.527727   \n\n          yield  \n0      4.773478  \n1      4.995000  \n2      5.217500  \n3      4.974500  \n4      5.010952  \n...         ...  \n77139  4.684737  \n77140  4.897368  \n77141  5.155000  \n77142  4.472174  \n77143  4.096818  \n\n[77144 rows x 26 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>date</th>\n      <th>ret</th>\n      <th>aum</th>\n      <th>companyid</th>\n      <th>Main Strategy</th>\n      <th>incentivefee</th>\n      <th>managementfee</th>\n      <th>year</th>\n      <th>month</th>\n      <th>...</th>\n      <th>SMB</th>\n      <th>HML</th>\n      <th>RF</th>\n      <th>PTFSBD</th>\n      <th>PTFSFX</th>\n      <th>PTFSCOM</th>\n      <th>Mom</th>\n      <th>DBAA</th>\n      <th>DGS10</th>\n      <th>yield</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4075</td>\n      <td>2006-03-31</td>\n      <td>0.021399</td>\n      <td>190388153</td>\n      <td>18038.0</td>\n      <td>European Long / Short</td>\n      <td>0.2</td>\n      <td>0.015</td>\n      <td>2006</td>\n      <td>3</td>\n      <td>...</td>\n      <td>3.44</td>\n      <td>0.60</td>\n      <td>0.37</td>\n      <td>-0.0662</td>\n      <td>-0.1037</td>\n      <td>-0.0589</td>\n      <td>1.26</td>\n      <td>6.411304</td>\n      <td>4.723913</td>\n      <td>4.773478</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4075</td>\n      <td>2006-05-31</td>\n      <td>0.007099</td>\n      <td>219482626</td>\n      <td>18038.0</td>\n      <td>European Long / Short</td>\n      <td>0.2</td>\n      <td>0.015</td>\n      <td>2006</td>\n      <td>5</td>\n      <td>...</td>\n      <td>-2.96</td>\n      <td>2.41</td>\n      <td>0.43</td>\n      <td>-0.0696</td>\n      <td>0.1569</td>\n      <td>-0.0457</td>\n      <td>-3.70</td>\n      <td>6.745455</td>\n      <td>5.110000</td>\n      <td>4.995000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4075</td>\n      <td>2006-07-31</td>\n      <td>-0.000299</td>\n      <td>230121070</td>\n      <td>18038.0</td>\n      <td>European Long / Short</td>\n      <td>0.2</td>\n      <td>0.015</td>\n      <td>2006</td>\n      <td>7</td>\n      <td>...</td>\n      <td>-3.98</td>\n      <td>2.60</td>\n      <td>0.40</td>\n      <td>-0.1733</td>\n      <td>-0.1157</td>\n      <td>-0.2304</td>\n      <td>-2.12</td>\n      <td>6.762000</td>\n      <td>5.087500</td>\n      <td>5.217500</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4075</td>\n      <td>2006-09-30</td>\n      <td>0.001799</td>\n      <td>233737989</td>\n      <td>18038.0</td>\n      <td>European Long / Short</td>\n      <td>0.2</td>\n      <td>0.015</td>\n      <td>2006</td>\n      <td>9</td>\n      <td>...</td>\n      <td>-1.36</td>\n      <td>0.08</td>\n      <td>0.41</td>\n      <td>-0.0386</td>\n      <td>-0.2034</td>\n      <td>0.0361</td>\n      <td>-0.96</td>\n      <td>6.430000</td>\n      <td>4.719000</td>\n      <td>4.974500</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4075</td>\n      <td>2006-11-30</td>\n      <td>0.007899</td>\n      <td>248747495</td>\n      <td>18038.0</td>\n      <td>European Long / Short</td>\n      <td>0.2</td>\n      <td>0.015</td>\n      <td>2006</td>\n      <td>11</td>\n      <td>...</td>\n      <td>0.70</td>\n      <td>0.14</td>\n      <td>0.42</td>\n      <td>-0.1081</td>\n      <td>0.4743</td>\n      <td>0.0003</td>\n      <td>-1.03</td>\n      <td>6.203810</td>\n      <td>4.595238</td>\n      <td>5.010952</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>77139</th>\n      <td>5832</td>\n      <td>2006-02-28</td>\n      <td>-0.075599</td>\n      <td>19079565</td>\n      <td>1622.0</td>\n      <td>Sector Commodity</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>2</td>\n      <td>...</td>\n      <td>-0.38</td>\n      <td>-0.34</td>\n      <td>0.34</td>\n      <td>-0.1832</td>\n      <td>-0.1338</td>\n      <td>-0.1007</td>\n      <td>-1.84</td>\n      <td>6.270000</td>\n      <td>4.568947</td>\n      <td>4.684737</td>\n    </tr>\n    <tr>\n      <th>77140</th>\n      <td>5832</td>\n      <td>2006-04-30</td>\n      <td>0.060999</td>\n      <td>18704055</td>\n      <td>1622.0</td>\n      <td>Sector Commodity</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>4</td>\n      <td>...</td>\n      <td>-1.42</td>\n      <td>2.34</td>\n      <td>0.36</td>\n      <td>0.1980</td>\n      <td>0.3177</td>\n      <td>0.2275</td>\n      <td>0.64</td>\n      <td>6.680526</td>\n      <td>4.990526</td>\n      <td>4.897368</td>\n    </tr>\n    <tr>\n      <th>77141</th>\n      <td>5832</td>\n      <td>2006-06-30</td>\n      <td>-0.011599</td>\n      <td>29194946</td>\n      <td>1622.0</td>\n      <td>Sector Commodity</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>6</td>\n      <td>...</td>\n      <td>-0.39</td>\n      <td>0.85</td>\n      <td>0.40</td>\n      <td>-0.1495</td>\n      <td>-0.1262</td>\n      <td>-0.0196</td>\n      <td>1.54</td>\n      <td>6.779545</td>\n      <td>5.106364</td>\n      <td>5.155000</td>\n    </tr>\n    <tr>\n      <th>77142</th>\n      <td>5832</td>\n      <td>2007-08-31</td>\n      <td>0.003099</td>\n      <td>135784198</td>\n      <td>1622.0</td>\n      <td>Sector Commodity</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2007</td>\n      <td>8</td>\n      <td>...</td>\n      <td>-0.12</td>\n      <td>-1.86</td>\n      <td>0.42</td>\n      <td>-0.0078</td>\n      <td>-0.0037</td>\n      <td>0.0807</td>\n      <td>0.10</td>\n      <td>6.652174</td>\n      <td>4.674783</td>\n      <td>4.472174</td>\n    </tr>\n    <tr>\n      <th>77143</th>\n      <td>5832</td>\n      <td>2007-10-31</td>\n      <td>0.042499</td>\n      <td>162374618</td>\n      <td>1622.0</td>\n      <td>Sector Commodity</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2007</td>\n      <td>10</td>\n      <td>...</td>\n      <td>0.08</td>\n      <td>-2.98</td>\n      <td>0.32</td>\n      <td>-0.1584</td>\n      <td>0.0497</td>\n      <td>0.1178</td>\n      <td>5.02</td>\n      <td>6.479545</td>\n      <td>4.527727</td>\n      <td>4.096818</td>\n    </tr>\n  </tbody>\n</table>\n<p>77144 rows × 26 columns</p>\n</div>"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tass4_pre"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T16:53:57.056151400Z",
     "start_time": "2024-08-07T16:53:56.974353400Z"
    }
   },
   "id": "b5c5b4a314f8c3de",
   "execution_count": 129
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hendrikv\\AppData\\Local\\Temp\\ipykernel_16636\\59595417.py:32: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-10.09793066 -10.09637028 -10.0968867  -10.12199551 -10.03167647\n",
      " -10.07762321 -10.084254   -10.02523413 -10.07321356 -10.10468779\n",
      " -10.12650267 -10.10750228 -10.10395663 -10.05716192 -10.04758774\n",
      " -10.10389236 -10.06539068 -10.11430598 -10.03381833 -10.10443111\n",
      " -10.11888117 -10.11678563 -10.106513  ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df['id'] == unique_id, 'excess_ret'] = df['lhs'] - predictions + df['alpha']\n"
     ]
    }
   ],
   "source": [
    "def asset_pricing(df, period_name):\n",
    "    df['lhs'] = df['ret_star'] - (df['yield']/100)\n",
    "    df['excess_ret'] = 0\n",
    "    df['beta1'] = 0.0\n",
    "    df['beta2'] = 0.0\n",
    "    df['beta3'] = 0.0\n",
    "    df['beta4'] = 0.0\n",
    "    df['beta5'] = 0.0\n",
    "    df['beta6'] = 0.0\n",
    "    df['beta7'] = 0.0\n",
    "    df['alpha'] = 0.0\n",
    "    df['stdv'] = 0.0\n",
    "    df['r2'] = 0.0\n",
    "\n",
    "    df.rename(columns={'mktrf': 'eq_prem'}, inplace=True)\n",
    "    unique_ids = df['id'].unique()\n",
    "\n",
    "    for unique_id in unique_ids:\n",
    "        sub_df = df[df['id'] == unique_id]\n",
    "        if len(sub_df) > 1:\n",
    "            model = sm.OLS(sub_df['lhs'], sm.add_constant(sub_df[['Mkt-RF', 'SMB', 'PTFSBD', 'PTFSFX', 'PTFSCOM', 'year', 'DBAA']])).fit()\n",
    "            df.loc[df['id'] == unique_id, 'r2'] = model.rsquared\n",
    "            predictions = model.predict(sm.add_constant(sub_df[['Mkt-RF', 'SMB', 'PTFSBD', 'PTFSFX', 'PTFSCOM', 'year', 'DBAA']]))\n",
    "            if len(model.params) > 1: df.loc[df['id'] == unique_id, 'beta1'] = model.params.iloc[1]\n",
    "            if len(model.params) > 2: df.loc[df['id'] == unique_id, 'beta2'] = model.params.iloc[2]\n",
    "            if len(model.params) > 3: df.loc[df['id'] == unique_id, 'beta3'] = model.params.iloc[3]\n",
    "            if len(model.params) > 4: df.loc[df['id'] == unique_id, 'beta4'] = model.params.iloc[4]\n",
    "            if len(model.params) > 5: df.loc[df['id'] == unique_id, 'beta5'] = model.params.iloc[5]\n",
    "            if len(model.params) > 6: df.loc[df['id'] == unique_id, 'beta6'] = model.params.iloc[6]\n",
    "            if len(model.params) > 7: df.loc[df['id'] == unique_id, 'beta7'] = model.params.iloc[7]\n",
    "            if len(model.params) > 0: df.loc[df['id'] == unique_id, 'alpha'] = model.params.iloc[0]\n",
    "            df.loc[df['id'] == unique_id, 'excess_ret'] = df['lhs'] - predictions + df['alpha']\n",
    "            df.loc[df['id'] == unique_id, 'stdv'] = df['excess_ret'].std()\n",
    "\n",
    "    df['excess_ret'] = df['excess_ret'].where(df['ret'].notna())\n",
    "    df['excess_ret'] = df['excess_ret'].where(df['alpha'].notna())\n",
    "    df.to_csv(f'tass5_{period_name}.csv', index=False)\n",
    "    return df\n",
    "\n",
    "# Perform asset pricing analysis for each period\n",
    "tass5_pre = asset_pricing(df_tass4_pre, 'pre')\n",
    "tass5_crisis = asset_pricing(df_tass4_crisis, 'crisis')\n",
    "tass5_post = asset_pricing(df_tass4_post, 'post')\n",
    "\n",
    "# Combine all periods into a single dataset\n",
    "tass5 = pd.concat([tass5_pre, tass5_crisis, tass5_post])\n",
    "tass5.to_csv('tass5.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T16:56:27.579100600Z",
     "start_time": "2024-08-07T16:53:57.058453600Z"
    }
   },
   "id": "2a4279672ff5af0c",
   "execution_count": 130
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "         id        date       ret        aum  companyid  \\\n0      4075  2006-03-31  0.021399  190388153    18038.0   \n1      4075  2006-05-31  0.007099  219482626    18038.0   \n2      4075  2006-07-31 -0.000299  230121070    18038.0   \n3      4075  2006-09-30  0.001799  233737989    18038.0   \n4      4075  2006-11-30  0.007899  248747495    18038.0   \n...     ...         ...       ...        ...        ...   \n52801  5832  2009-09-30  0.021599  108657603     1622.0   \n52802  5832  2009-11-30  0.045399  109138852     1622.0   \n52803  5832  2011-01-31 -0.007399   73223532     1622.0   \n52804  5832  2010-03-31  0.014999  113528132     1622.0   \n52805  5832  2010-05-31 -0.064499  116156082     1622.0   \n\n               Main Strategy  incentivefee managementfee  year  month  ...  \\\n0      European Long / Short           0.2         0.015  2006      3  ...   \n1      European Long / Short           0.2         0.015  2006      5  ...   \n2      European Long / Short           0.2         0.015  2006      7  ...   \n3      European Long / Short           0.2         0.015  2006      9  ...   \n4      European Long / Short           0.2         0.015  2006     11  ...   \n...                      ...           ...           ...   ...    ...  ...   \n52801       Sector Commodity           NaN           NaN  2009      9  ...   \n52802       Sector Commodity           NaN           NaN  2009     11  ...   \n52803       Sector Commodity           NaN           NaN  2011      1  ...   \n52804       Sector Commodity           NaN           NaN  2010      3  ...   \n52805       Sector Commodity           NaN           NaN  2010      5  ...   \n\n          beta1     beta2     beta3     beta4     beta5     beta6     beta7  \\\n0           NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n1           NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n2           NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n3           NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n4           NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n...         ...       ...       ...       ...       ...       ...       ...   \n52801  0.004164 -0.000057  0.017579 -0.008402  0.064152 -0.002356  0.001147   \n52802  0.004164 -0.000057  0.017579 -0.008402  0.064152 -0.002356  0.001147   \n52803  0.004164 -0.000057  0.017579 -0.008402  0.064152 -0.002356  0.001147   \n52804  0.004164 -0.000057  0.017579 -0.008402  0.064152 -0.002356  0.001147   \n52805  0.004164 -0.000057  0.017579 -0.008402  0.064152 -0.002356  0.001147   \n\n          alpha       stdv        r2  \n0           NaN   0.000000       NaN  \n1           NaN   0.000000       NaN  \n2           NaN   0.000000       NaN  \n3           NaN   0.000000       NaN  \n4           NaN   0.000000       NaN  \n...         ...        ...       ...  \n52801  4.723873  34.410168  0.633671  \n52802  4.723873  34.410168  0.633671  \n52803  4.723873  34.410168  0.633671  \n52804  4.723873  34.410168  0.633671  \n52805  4.723873  34.410168  0.633671  \n\n[162471 rows x 38 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>date</th>\n      <th>ret</th>\n      <th>aum</th>\n      <th>companyid</th>\n      <th>Main Strategy</th>\n      <th>incentivefee</th>\n      <th>managementfee</th>\n      <th>year</th>\n      <th>month</th>\n      <th>...</th>\n      <th>beta1</th>\n      <th>beta2</th>\n      <th>beta3</th>\n      <th>beta4</th>\n      <th>beta5</th>\n      <th>beta6</th>\n      <th>beta7</th>\n      <th>alpha</th>\n      <th>stdv</th>\n      <th>r2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4075</td>\n      <td>2006-03-31</td>\n      <td>0.021399</td>\n      <td>190388153</td>\n      <td>18038.0</td>\n      <td>European Long / Short</td>\n      <td>0.2</td>\n      <td>0.015</td>\n      <td>2006</td>\n      <td>3</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4075</td>\n      <td>2006-05-31</td>\n      <td>0.007099</td>\n      <td>219482626</td>\n      <td>18038.0</td>\n      <td>European Long / Short</td>\n      <td>0.2</td>\n      <td>0.015</td>\n      <td>2006</td>\n      <td>5</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4075</td>\n      <td>2006-07-31</td>\n      <td>-0.000299</td>\n      <td>230121070</td>\n      <td>18038.0</td>\n      <td>European Long / Short</td>\n      <td>0.2</td>\n      <td>0.015</td>\n      <td>2006</td>\n      <td>7</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4075</td>\n      <td>2006-09-30</td>\n      <td>0.001799</td>\n      <td>233737989</td>\n      <td>18038.0</td>\n      <td>European Long / Short</td>\n      <td>0.2</td>\n      <td>0.015</td>\n      <td>2006</td>\n      <td>9</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4075</td>\n      <td>2006-11-30</td>\n      <td>0.007899</td>\n      <td>248747495</td>\n      <td>18038.0</td>\n      <td>European Long / Short</td>\n      <td>0.2</td>\n      <td>0.015</td>\n      <td>2006</td>\n      <td>11</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>52801</th>\n      <td>5832</td>\n      <td>2009-09-30</td>\n      <td>0.021599</td>\n      <td>108657603</td>\n      <td>1622.0</td>\n      <td>Sector Commodity</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2009</td>\n      <td>9</td>\n      <td>...</td>\n      <td>0.004164</td>\n      <td>-0.000057</td>\n      <td>0.017579</td>\n      <td>-0.008402</td>\n      <td>0.064152</td>\n      <td>-0.002356</td>\n      <td>0.001147</td>\n      <td>4.723873</td>\n      <td>34.410168</td>\n      <td>0.633671</td>\n    </tr>\n    <tr>\n      <th>52802</th>\n      <td>5832</td>\n      <td>2009-11-30</td>\n      <td>0.045399</td>\n      <td>109138852</td>\n      <td>1622.0</td>\n      <td>Sector Commodity</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2009</td>\n      <td>11</td>\n      <td>...</td>\n      <td>0.004164</td>\n      <td>-0.000057</td>\n      <td>0.017579</td>\n      <td>-0.008402</td>\n      <td>0.064152</td>\n      <td>-0.002356</td>\n      <td>0.001147</td>\n      <td>4.723873</td>\n      <td>34.410168</td>\n      <td>0.633671</td>\n    </tr>\n    <tr>\n      <th>52803</th>\n      <td>5832</td>\n      <td>2011-01-31</td>\n      <td>-0.007399</td>\n      <td>73223532</td>\n      <td>1622.0</td>\n      <td>Sector Commodity</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2011</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.004164</td>\n      <td>-0.000057</td>\n      <td>0.017579</td>\n      <td>-0.008402</td>\n      <td>0.064152</td>\n      <td>-0.002356</td>\n      <td>0.001147</td>\n      <td>4.723873</td>\n      <td>34.410168</td>\n      <td>0.633671</td>\n    </tr>\n    <tr>\n      <th>52804</th>\n      <td>5832</td>\n      <td>2010-03-31</td>\n      <td>0.014999</td>\n      <td>113528132</td>\n      <td>1622.0</td>\n      <td>Sector Commodity</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2010</td>\n      <td>3</td>\n      <td>...</td>\n      <td>0.004164</td>\n      <td>-0.000057</td>\n      <td>0.017579</td>\n      <td>-0.008402</td>\n      <td>0.064152</td>\n      <td>-0.002356</td>\n      <td>0.001147</td>\n      <td>4.723873</td>\n      <td>34.410168</td>\n      <td>0.633671</td>\n    </tr>\n    <tr>\n      <th>52805</th>\n      <td>5832</td>\n      <td>2010-05-31</td>\n      <td>-0.064499</td>\n      <td>116156082</td>\n      <td>1622.0</td>\n      <td>Sector Commodity</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2010</td>\n      <td>5</td>\n      <td>...</td>\n      <td>0.004164</td>\n      <td>-0.000057</td>\n      <td>0.017579</td>\n      <td>-0.008402</td>\n      <td>0.064152</td>\n      <td>-0.002356</td>\n      <td>0.001147</td>\n      <td>4.723873</td>\n      <td>34.410168</td>\n      <td>0.633671</td>\n    </tr>\n  </tbody>\n</table>\n<p>162471 rows × 38 columns</p>\n</div>"
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tass5"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T16:56:27.763464200Z",
     "start_time": "2024-08-07T16:56:27.582440700Z"
    }
   },
   "id": "e9d17ec3f0819d7b",
   "execution_count": 131
  },
  {
   "cell_type": "markdown",
   "source": [
    "TRYING TO FIX ALPHA IN CELL BELOW\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1dece4b534c3ab5b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hendrikv\\AppData\\Local\\Temp\\ipykernel_16636\\349969561.py:32: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.02962226  0.02869303  0.02879245  0.00131779  0.09115733  0.04934549\n",
      "  0.04085805  0.09496932  0.04808185  0.01792507 -0.00786802  0.01257524\n",
      "  0.02486574  0.07120228  0.07407161  0.02026662  0.05469123  0.01022664\n",
      "  0.0901933   0.01885028  0.00460665  0.00313359  0.01581107]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df['id'] == unique_id, 'excess_ret'] = df['lhs'] - predictions + df['alpha']\n"
     ]
    }
   ],
   "source": [
    "def asset_pricing(df, period_name):\n",
    "    df['lhs'] = df['ret_star'] - (df['yield']/100)\n",
    "    df['excess_ret'] = 0\n",
    "    df['beta1'] = 0.0\n",
    "    df['beta2'] = 0.0\n",
    "    df['beta3'] = 0.0\n",
    "    df['beta4'] = 0.0\n",
    "    df['beta5'] = 0.0\n",
    "    df['beta6'] = 0.0\n",
    "    df['beta7'] = 0.0\n",
    "    df['alpha'] = 0.0\n",
    "    df['stdv'] = 0.0\n",
    "    df['r2'] = 0.0\n",
    "\n",
    "    df.rename(columns={'mktrf': 'eq_prem'}, inplace=True)\n",
    "    unique_ids = df['id'].unique()\n",
    "\n",
    "    for unique_id in unique_ids:\n",
    "        sub_df = df[df['id'] == unique_id]\n",
    "        if len(sub_df) > 1:\n",
    "            model = sm.OLS(sub_df['lhs'], sm.add_constant(sub_df[['Mkt-RF', 'SMB', 'PTFSBD', 'PTFSFX', 'PTFSCOM', 'DBAA']])).fit()\n",
    "            df.loc[df['id'] == unique_id, 'r2'] = model.rsquared\n",
    "            predictions = model.predict(sm.add_constant(sub_df[['Mkt-RF', 'SMB', 'PTFSBD', 'PTFSFX', 'PTFSCOM', 'DBAA']]))\n",
    "            if len(model.params) > 1: df.loc[df['id'] == unique_id, 'beta1'] = model.params.iloc[1]\n",
    "            if len(model.params) > 2: df.loc[df['id'] == unique_id, 'beta2'] = model.params.iloc[2]\n",
    "            if len(model.params) > 3: df.loc[df['id'] == unique_id, 'beta3'] = model.params.iloc[3]\n",
    "            if len(model.params) > 4: df.loc[df['id'] == unique_id, 'beta4'] = model.params.iloc[4]\n",
    "            if len(model.params) > 5: df.loc[df['id'] == unique_id, 'beta5'] = model.params.iloc[5]\n",
    "            if len(model.params) > 6: df.loc[df['id'] == unique_id, 'beta6'] = model.params.iloc[6]\n",
    "            if len(model.params) > 7: df.loc[df['id'] == unique_id, 'beta7'] = model.params.iloc[7]\n",
    "            if len(model.params) > 0: df.loc[df['id'] == unique_id, 'alpha'] = model.params.iloc[0]\n",
    "            df.loc[df['id'] == unique_id, 'excess_ret'] = df['lhs'] - predictions + df['alpha']\n",
    "            df.loc[df['id'] == unique_id, 'stdv'] = df['excess_ret'].std()\n",
    "\n",
    "    df['excess_ret'] = df['excess_ret'].where(df['ret'].notna())\n",
    "    df['excess_ret'] = df['excess_ret'].where(df['alpha'].notna())\n",
    "    df.to_csv(f'tass5_{period_name}.csv', index=False)\n",
    "    return df\n",
    "\n",
    "# Perform asset pricing analysis for each period\n",
    "tass5_pre = asset_pricing(df_tass4_pre, 'pre')\n",
    "tass5_crisis = asset_pricing(df_tass4_crisis, 'crisis')\n",
    "tass5_post = asset_pricing(df_tass4_post, 'post')\n",
    "\n",
    "# Combine all periods into a single dataset\n",
    "# Combine all periods into a single dataset\n",
    "tass5 = pd.concat([tass5_pre, tass5_crisis, tass5_post])\n",
    "tass5.to_csv('tass5.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T16:58:48.245100100Z",
     "start_time": "2024-08-07T16:56:27.773245100Z"
    }
   },
   "id": "371350f7de12d061",
   "execution_count": 132
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "         id        date       ret        aum  companyid  \\\n0      4075  2006-03-31  0.021399  190388153    18038.0   \n1      4075  2006-05-31  0.007099  219482626    18038.0   \n2      4075  2006-07-31 -0.000299  230121070    18038.0   \n3      4075  2006-09-30  0.001799  233737989    18038.0   \n4      4075  2006-11-30  0.007899  248747495    18038.0   \n...     ...         ...       ...        ...        ...   \n52801  5832  2009-09-30  0.021599  108657603     1622.0   \n52802  5832  2009-11-30  0.045399  109138852     1622.0   \n52803  5832  2011-01-31 -0.007399   73223532     1622.0   \n52804  5832  2010-03-31  0.014999  113528132     1622.0   \n52805  5832  2010-05-31 -0.064499  116156082     1622.0   \n\n               Main Strategy  incentivefee managementfee  year  month  ...  \\\n0      European Long / Short           0.2         0.015  2006      3  ...   \n1      European Long / Short           0.2         0.015  2006      5  ...   \n2      European Long / Short           0.2         0.015  2006      7  ...   \n3      European Long / Short           0.2         0.015  2006      9  ...   \n4      European Long / Short           0.2         0.015  2006     11  ...   \n...                      ...           ...           ...   ...    ...  ...   \n52801       Sector Commodity           NaN           NaN  2009      9  ...   \n52802       Sector Commodity           NaN           NaN  2009     11  ...   \n52803       Sector Commodity           NaN           NaN  2011      1  ...   \n52804       Sector Commodity           NaN           NaN  2010      3  ...   \n52805       Sector Commodity           NaN           NaN  2010      5  ...   \n\n         beta1     beta2     beta3     beta4     beta5     beta6  beta7  \\\n0          NaN       NaN       NaN       NaN       NaN       NaN    0.0   \n1          NaN       NaN       NaN       NaN       NaN       NaN    0.0   \n2          NaN       NaN       NaN       NaN       NaN       NaN    0.0   \n3          NaN       NaN       NaN       NaN       NaN       NaN    0.0   \n4          NaN       NaN       NaN       NaN       NaN       NaN    0.0   \n...        ...       ...       ...       ...       ...       ...    ...   \n52801  0.00422 -0.000045  0.020226 -0.008122  0.065248  0.004681    0.0   \n52802  0.00422 -0.000045  0.020226 -0.008122  0.065248  0.004681    0.0   \n52803  0.00422 -0.000045  0.020226 -0.008122  0.065248  0.004681    0.0   \n52804  0.00422 -0.000045  0.020226 -0.008122  0.065248  0.004681    0.0   \n52805  0.00422 -0.000045  0.020226 -0.008122  0.065248  0.004681    0.0   \n\n          alpha      stdv        r2  \n0           NaN  0.000000       NaN  \n1           NaN  0.000000       NaN  \n2           NaN  0.000000       NaN  \n3           NaN  0.000000       NaN  \n4           NaN  0.000000       NaN  \n...         ...       ...       ...  \n52801 -0.033813  0.236887  0.631183  \n52802 -0.033813  0.236887  0.631183  \n52803 -0.033813  0.236887  0.631183  \n52804 -0.033813  0.236887  0.631183  \n52805 -0.033813  0.236887  0.631183  \n\n[162471 rows x 38 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>date</th>\n      <th>ret</th>\n      <th>aum</th>\n      <th>companyid</th>\n      <th>Main Strategy</th>\n      <th>incentivefee</th>\n      <th>managementfee</th>\n      <th>year</th>\n      <th>month</th>\n      <th>...</th>\n      <th>beta1</th>\n      <th>beta2</th>\n      <th>beta3</th>\n      <th>beta4</th>\n      <th>beta5</th>\n      <th>beta6</th>\n      <th>beta7</th>\n      <th>alpha</th>\n      <th>stdv</th>\n      <th>r2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4075</td>\n      <td>2006-03-31</td>\n      <td>0.021399</td>\n      <td>190388153</td>\n      <td>18038.0</td>\n      <td>European Long / Short</td>\n      <td>0.2</td>\n      <td>0.015</td>\n      <td>2006</td>\n      <td>3</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4075</td>\n      <td>2006-05-31</td>\n      <td>0.007099</td>\n      <td>219482626</td>\n      <td>18038.0</td>\n      <td>European Long / Short</td>\n      <td>0.2</td>\n      <td>0.015</td>\n      <td>2006</td>\n      <td>5</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4075</td>\n      <td>2006-07-31</td>\n      <td>-0.000299</td>\n      <td>230121070</td>\n      <td>18038.0</td>\n      <td>European Long / Short</td>\n      <td>0.2</td>\n      <td>0.015</td>\n      <td>2006</td>\n      <td>7</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4075</td>\n      <td>2006-09-30</td>\n      <td>0.001799</td>\n      <td>233737989</td>\n      <td>18038.0</td>\n      <td>European Long / Short</td>\n      <td>0.2</td>\n      <td>0.015</td>\n      <td>2006</td>\n      <td>9</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4075</td>\n      <td>2006-11-30</td>\n      <td>0.007899</td>\n      <td>248747495</td>\n      <td>18038.0</td>\n      <td>European Long / Short</td>\n      <td>0.2</td>\n      <td>0.015</td>\n      <td>2006</td>\n      <td>11</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>52801</th>\n      <td>5832</td>\n      <td>2009-09-30</td>\n      <td>0.021599</td>\n      <td>108657603</td>\n      <td>1622.0</td>\n      <td>Sector Commodity</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2009</td>\n      <td>9</td>\n      <td>...</td>\n      <td>0.00422</td>\n      <td>-0.000045</td>\n      <td>0.020226</td>\n      <td>-0.008122</td>\n      <td>0.065248</td>\n      <td>0.004681</td>\n      <td>0.0</td>\n      <td>-0.033813</td>\n      <td>0.236887</td>\n      <td>0.631183</td>\n    </tr>\n    <tr>\n      <th>52802</th>\n      <td>5832</td>\n      <td>2009-11-30</td>\n      <td>0.045399</td>\n      <td>109138852</td>\n      <td>1622.0</td>\n      <td>Sector Commodity</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2009</td>\n      <td>11</td>\n      <td>...</td>\n      <td>0.00422</td>\n      <td>-0.000045</td>\n      <td>0.020226</td>\n      <td>-0.008122</td>\n      <td>0.065248</td>\n      <td>0.004681</td>\n      <td>0.0</td>\n      <td>-0.033813</td>\n      <td>0.236887</td>\n      <td>0.631183</td>\n    </tr>\n    <tr>\n      <th>52803</th>\n      <td>5832</td>\n      <td>2011-01-31</td>\n      <td>-0.007399</td>\n      <td>73223532</td>\n      <td>1622.0</td>\n      <td>Sector Commodity</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2011</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.00422</td>\n      <td>-0.000045</td>\n      <td>0.020226</td>\n      <td>-0.008122</td>\n      <td>0.065248</td>\n      <td>0.004681</td>\n      <td>0.0</td>\n      <td>-0.033813</td>\n      <td>0.236887</td>\n      <td>0.631183</td>\n    </tr>\n    <tr>\n      <th>52804</th>\n      <td>5832</td>\n      <td>2010-03-31</td>\n      <td>0.014999</td>\n      <td>113528132</td>\n      <td>1622.0</td>\n      <td>Sector Commodity</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2010</td>\n      <td>3</td>\n      <td>...</td>\n      <td>0.00422</td>\n      <td>-0.000045</td>\n      <td>0.020226</td>\n      <td>-0.008122</td>\n      <td>0.065248</td>\n      <td>0.004681</td>\n      <td>0.0</td>\n      <td>-0.033813</td>\n      <td>0.236887</td>\n      <td>0.631183</td>\n    </tr>\n    <tr>\n      <th>52805</th>\n      <td>5832</td>\n      <td>2010-05-31</td>\n      <td>-0.064499</td>\n      <td>116156082</td>\n      <td>1622.0</td>\n      <td>Sector Commodity</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2010</td>\n      <td>5</td>\n      <td>...</td>\n      <td>0.00422</td>\n      <td>-0.000045</td>\n      <td>0.020226</td>\n      <td>-0.008122</td>\n      <td>0.065248</td>\n      <td>0.004681</td>\n      <td>0.0</td>\n      <td>-0.033813</td>\n      <td>0.236887</td>\n      <td>0.631183</td>\n    </tr>\n  </tbody>\n</table>\n<p>162471 rows × 38 columns</p>\n</div>"
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tass5"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T16:58:48.428478Z",
     "start_time": "2024-08-07T16:58:48.249165100Z"
    }
   },
   "id": "4370e6e85dcb6e9a",
   "execution_count": 133
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Preliminarily define \"closed\" funds during the crisis\n",
    "tass5 = pd.concat([tass5_pre, tass5_crisis, tass5_post])\n",
    "\n",
    "# Define the crisis and post periods\n",
    "tass5['crisis'] = 0\n",
    "tass5.loc[(tass5['mydate'] >= 573) & (tass5['mydate'] <= 594), 'crisis'] = 1\n",
    "\n",
    "tass5['post'] = 0\n",
    "tass5.loc[tass5['mydate'] > 594, 'post'] = 1\n",
    "\n",
    "# Identify the max and min mydate for each fund\n",
    "tass5['max_mydate'] = tass5.groupby('id')['mydate'].transform('max')\n",
    "tass5['min_mydate'] = tass5.groupby('id')['mydate'].transform('min')\n",
    "\n",
    "# Define funds that closed during the crisis\n",
    "tass5['closedxcrisis'] = 0\n",
    "tass5.loc[(tass5['max_mydate'] >= 573) & (tass5['max_mydate'] <= 594) & (tass5['mydate'] == tass5['max_mydate']), 'closedxcrisis'] = 1\n",
    "\n",
    "# Define firms that closed at least one fund during the crisis\n",
    "tass5['firm_closedxcrisis'] = tass5.groupby('companyid')['closedxcrisis'].transform('max')\n",
    "\n",
    "# Save the potential treatment dataset\n",
    "tass5.to_csv('potential_treat0.csv', index=False)\n",
    "\n",
    "# Create a dataset of firms that closed at least one fund during the crisis\n",
    "potential_divcorr = tass5[tass5['firm_closedxcrisis'] == 1].drop(columns=['firm_closedxcrisis', 'closedxcrisis'])\n",
    "potential_divcorr.to_csv('potential_divcorr.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T16:58:57.125407900Z",
     "start_time": "2024-08-07T16:58:48.432870200Z"
    }
   },
   "id": "2b81de57e704d968",
   "execution_count": 134
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Pre-define treatment\n",
    "diag1 = tass5.groupby('companyid').agg({'firm_closedxcrisis': 'max', 'mydate': 'max'}).reset_index()\n",
    "diag1.rename(columns={'firm_closedxcrisis': 'treated'}, inplace=True)\n",
    "diag1['firm_closed'] = 0\n",
    "diag1.loc[(diag1['mydate'] >= 573) & (diag1['mydate'] <= 593), 'firm_closed'] = 1\n",
    "diag1.to_csv('diag1.csv', index=False)\n",
    "\n",
    "diag2 = tass5.merge(diag1, on='companyid', how='left')\n",
    "diag2['mydate'] = diag2['mydate_x']\n",
    "diag2['mydate2'] = diag2['mydate']\n",
    "diag2 = diag2.groupby('id').agg({'firm_closed': 'max', 'closedxcrisis': 'max', 'treated': 'max', 'mydate': 'max', 'mydate2': 'min'}).reset_index()\n",
    "diag2['pre_treat'] = 0\n",
    "diag2.loc[(diag2['firm_closed'] == 0) & (diag2['treated'] == 1) & (diag2['closedxcrisis'] == 0) & (diag2['mydate2'] >= 562) & (diag2['mydate'] >= 605), 'pre_treat'] = 1\n",
    "diag2 = diag2[['id', 'pre_treat']]\n",
    "diag2.to_csv('diag2.csv', index=False)\n",
    "\n",
    "potential_treat0 = pd.read_csv('potential_treat0.csv')\n",
    "diag2 = pd.read_csv('diag2.csv')\n",
    "\n",
    "potential_treat0 = potential_treat0.merge(diag2, on='id', how='left').fillna(0)\n",
    "potential_treat0['pre_treat'] = potential_treat0['pre_treat'].astype(int)\n",
    "diag3 = potential_treat0.groupby('companyid').agg({'pre_treat': 'max'}).reset_index()\n",
    "diag3.to_csv('diag3.csv', index=False)\n",
    "\n",
    "# Clean up intermediate files\n",
    "import os\n",
    "\n",
    "files_to_delete = [\n",
    "    'dataff_fffactors.csv', 'dataff_fung_hsieh.csv', 'dataff_mom.csv', \n",
    "    'dataff_bond.csv', 'dataff_credit.csv', 'tass4_pre.csv', \n",
    "    'tass4_crisis.csv', 'tass4_post.csv', 'tass5_pre.csv', \n",
    "    'tass5_crisis.csv', 'tass5_post.csv'\n",
    "]\n",
    "\n",
    "for file in files_to_delete:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T16:58:58.841474400Z",
     "start_time": "2024-08-07T16:58:57.132913900Z"
    }
   },
   "id": "c383f3d88e8c307d",
   "execution_count": 135
  },
  {
   "cell_type": "markdown",
   "source": [
    "CAR36"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ddcb8b07e6a1991"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('tass5.csv')\n",
    "\n",
    "# Ensure the dataframe is sorted by 'id' and 'mydate'\n",
    "df = df.sort_values(by=['id', 'mydate'])\n",
    "\n",
    "# Calculate the moving average of excess returns over the past 12 months\n",
    "df['moveave_ex'] = df.groupby('id')['excess_ret'].transform(lambda x: x.rolling(window=12, min_periods=1).mean())\n",
    "\n",
    "# Calculate the sum of squared deviations from the moving average\n",
    "def sum_sq_ex(group):\n",
    "    return ((group['excess_ret'] - group['moveave_ex'])**2).rolling(window=12, min_periods=1).sum()\n",
    "\n",
    "df['sum_sq_ex'] = df.groupby('id').apply(sum_sq_ex).reset_index(level=0, drop=True)\n",
    "\n",
    "# Calculate the variance of the moving average and the standard deviation\n",
    "df['var_ex_move'] = df['sum_sq_ex'] / 12\n",
    "df['stdv_ex_move'] = np.sqrt(df['var_ex_move'])\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['sum_sq_ex', 'var_ex_move'])\n",
    "\n",
    "# Generate a counter for each id\n",
    "df['counter'] = df.groupby('id').cumcount() + 1\n",
    "\n",
    "# Generate the retdum column\n",
    "df['retdum'] = np.where(df['excess_ret'].notna(), 1, 0)\n",
    "\n",
    "# Calculate retcounter for various lengths up to 36\n",
    "for i in range(1, 37):\n",
    "    df[f'retcounter{i}'] = df.groupby('id')['retdum'].transform(lambda x: x.shift(i).rolling(window=i, min_periods=1).sum())\n",
    "\n",
    "# Determine the maximum retcounter value for each row\n",
    "df['retcounter'] = df[[f'retcounter{i}' for i in range(1, 37)]].bfill(axis=1).iloc[:, 0]\n",
    "\n",
    "# Calculate the CAR values for various lengths up to 36\n",
    "for i in range(1, 37):\n",
    "    df[f'CAR{i}'] = df.groupby('id')['excess_ret'].transform(lambda x: x.shift(i).rolling(window=i, min_periods=1).sum())\n",
    "\n",
    "# Determine the appropriate CAR value based on retcounter\n",
    "df['CARstar'] = np.nan\n",
    "for i in range(1, 37):\n",
    "    df['CARstar'] = np.where(df['retcounter'] >= i, df[f'CAR{i}'], df['CARstar'])\n",
    "\n",
    "# Adjust retcounter values greater than 36 to 36\n",
    "df['retcounter'] = np.where(df['retcounter'] > 36, 36, df['retcounter'])\n",
    "\n",
    "# Calculate the average CAR over the period\n",
    "df['avgCAR36'] = df['CARstar'] / df['retcounter']\n",
    "\n",
    "# Save the intermediate result to a CSV file\n",
    "df.to_csv('car36a.csv', index=False)\n",
    "\n",
    "# Filter out rows where excess is missing\n",
    "df = df[df['excess_ret'].notna()]\n",
    "\n",
    "# Keep only necessary columns\n",
    "df = df[['id', 'mydate', 'avgCAR36', 'stdv_ex_move']]\n",
    "\n",
    "# Save the final result to a CSV file\n",
    "df.to_csv('car36.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T17:01:27.017635200Z",
     "start_time": "2024-08-07T16:58:58.851408500Z"
    }
   },
   "id": "b48281ac24d9b559",
   "execution_count": 136
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T17:01:27.027875500Z",
     "start_time": "2024-08-07T17:01:27.021429100Z"
    }
   },
   "id": "79f8dc71f5d8a001",
   "execution_count": 136
  },
  {
   "cell_type": "markdown",
   "source": [
    "ALL_CLOSURES"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b8c779fd3864bc7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the mydate converter\n",
    "mydate_converter = pd.read_csv('mydate_converter.csv', header=None, names=['year', 'month', 'mydate'], skiprows=[0])\n",
    "\n",
    "# Load the tass5 dataset\n",
    "tass5 = pd.read_csv('tass5.csv')\n",
    "\n",
    "# Calculate min and max mydate for each fund\n",
    "tass5['min_mydate'] = tass5.groupby('id')['mydate'].transform('min')\n",
    "tass5['max_mydate'] = tass5.groupby('id')['mydate'].transform('max')\n",
    "\n",
    "# Generate the starts data\n",
    "starts = tass5[['id', 'min_mydate']].drop_duplicates()\n",
    "starts = starts.groupby('min_mydate').size().reset_index(name='starts')\n",
    "starts.rename(columns={'min_mydate': 'mydate'}, inplace=True)\n",
    "\n",
    "# Generate the closures data\n",
    "closures = tass5[['id', 'max_mydate']].drop_duplicates()\n",
    "closures = closures.groupby('max_mydate').size().reset_index(name='closures')\n",
    "closures.rename(columns={'max_mydate': 'mydate'}, inplace=True)\n",
    "\n",
    "# Merge starts and closures data\n",
    "open_close = pd.merge(starts, closures, on='mydate', how='outer').fillna(0)\n",
    "\n",
    "# Merge with mydate converter\n",
    "open_close = pd.merge(open_close, mydate_converter, on='mydate', how='left')\n",
    "\n",
    "# Save the open_close data to Excel\n",
    "open_close.to_excel('open_close_TASS2015.xlsx', index=False)\n",
    "\n",
    "# Aggregate data by year\n",
    "open_close_year = open_close.groupby('year').sum().reset_index()\n",
    "open_close_year = open_close_year[['year', 'starts', 'closures']]\n",
    "\n",
    "# Save the open_close_year data to Excel\n",
    "open_close_year.to_excel('open_close_TASS2015_year.xlsx', index=False)\n",
    "\n",
    "# Generate the all_close_treat data\n",
    "tass5 = tass5[['id', 'companyid', 'mydate', 'min_mydate', 'max_mydate']].drop_duplicates()\n",
    "tass5['closed'] = np.where((tass5['mydate'] == tass5['max_mydate']) & (tass5['max_mydate'] <= 653), 1, 0)\n",
    "tass5['opened'] = np.where((tass5['mydate'] == tass5['min_mydate']) & (tass5['min_mydate'] >= 409), 1, 0)\n",
    "\n",
    "# Collapse data by companyid and mydate\n",
    "collapsed_data = tass5.groupby(['companyid', 'mydate']).sum().reset_index()\n",
    "\n",
    "# Create lagged closed variables\n",
    "for i in range(1, 31):\n",
    "    collapsed_data[f'L{i}closed'] = collapsed_data.groupby('companyid')['closed'].shift(i).fillna(0)\n",
    "\n",
    "# Calculate net openings and closings\n",
    "collapsed_data['net'] = collapsed_data['opened'] - collapsed_data[[f'L{i}closed' for i in range(1, 13)]].sum(axis=1)\n",
    "\n",
    "# Create open_close and all_close_treat variables\n",
    "collapsed_data['open_close'] = np.where((collapsed_data['opened'] == 1) & (collapsed_data['net'] <= 0), 1, 0)\n",
    "collapsed_data['all_close_treat'] = collapsed_data[[f'L{i}closed' for i in range(1, 31)]].max(axis=1)\n",
    "collapsed_data['time_treat_all'] = np.argmax(collapsed_data[[f'L{i}closed' for i in range(1, 31)]].values >= 1, axis=1) + 1\n",
    "collapsed_data['time_treat_all'] = np.where(collapsed_data['all_close_treat'] >= 1, collapsed_data['time_treat_all'], 0)\n",
    "\n",
    "# Keep necessary columns and sort\n",
    "collapsed_data = collapsed_data[['companyid', 'mydate', 'all_close_treat', 'time_treat_all']].sort_values(by=['companyid', 'mydate'])\n",
    "\n",
    "# Save the all_close_treat data\n",
    "collapsed_data.to_csv('all_close_treat.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T17:01:29.294132100Z",
     "start_time": "2024-08-07T17:01:27.039489500Z"
    }
   },
   "id": "3aba75e1e07f76c6",
   "execution_count": 137
  },
  {
   "cell_type": "markdown",
   "source": [
    "RET_CORR_PAIRS"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "775a2e45520f8dfa"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the potential_divcorr dataset\n",
    "potential_divcorr = pd.read_csv('potential_divcorr.csv')\n",
    "\n",
    "# Step 1: Count the number of funds per firm at any given point in time (simulcounter)\n",
    "potential_divcorr['simulcounter'] = potential_divcorr.groupby(['companyid', 'mydate']).cumcount() + 1\n",
    "potential_divcorr['maxsimulcounter'] = potential_divcorr.groupby('companyid')['simulcounter'].transform('max')\n",
    "simulcounter_df = potential_divcorr.drop_duplicates(subset=['companyid']).copy()\n",
    "simulcounter_df = simulcounter_df[['companyid', 'maxsimulcounter']].rename(columns={'maxsimulcounter': 'simulcounter'})\n",
    "# Save simulcounter data\n",
    "simulcounter_df.to_csv('simulcounter.csv', index=False)\n",
    "# Step 2: Count the number of funds per firm and filter firms with at least two funds simultaneously\n",
    "potential_divcorr = pd.merge(potential_divcorr, simulcounter_df, on='companyid', how='inner')\n",
    "potential_divcorr['simulcounter'] = potential_divcorr['simulcounter_y']\n",
    "potential_divcorr = potential_divcorr[potential_divcorr['simulcounter'] > 1]\n",
    "\n",
    "# Step 3: Create a sequential fund identifier within the firm (fundcounter)\n",
    "fundcounter_df = potential_divcorr.groupby('id').agg({'companyid': 'max', 'mydate': 'min'}).reset_index()\n",
    "fundcounter_df = fundcounter_df.sort_values(by=['companyid', 'mydate', 'id'])\n",
    "fundcounter_df['fund_counter'] = fundcounter_df.groupby('companyid').cumcount() + 1\n",
    "fundcounter_df = fundcounter_df[['companyid', 'id', 'fund_counter']]\n",
    "\n",
    "# Save fundcounter data\n",
    "fundcounter_df.to_csv('fundcounter.csv', index=False)\n",
    "\n",
    "# Step 4: Create a database of error terms from the 7-factor regressions\n",
    "potential_divcorr['epsilon'] = potential_divcorr['excess_ret'] - potential_divcorr['alpha']\n",
    "epsilon_df = potential_divcorr[['id', 'mydate', 'epsilon']]\n",
    "\n",
    "# Save epsilon data\n",
    "epsilon_df.to_csv('epsilon.csv', index=False)\n",
    "potential_divcorr.to_csv('potential_divcorr.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T17:01:30.037967800Z",
     "start_time": "2024-08-07T17:01:29.303722900Z"
    }
   },
   "id": "f752094e59adfb00",
   "execution_count": 138
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T17:01:30.043952100Z",
     "start_time": "2024-08-07T17:01:30.040201500Z"
    }
   },
   "id": "6dba2ba88b23744c",
   "execution_count": 138
  },
  {
   "cell_type": "markdown",
   "source": [
    "RET_CORR_BIG"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d181b256883a4d83"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "import glob\n",
    "import logging\n",
    "\n",
    "# Initialize the logging functionality\n",
    "logging.basicConfig(filename='ret_corr_big.log', level=logging.INFO)\n",
    "\n",
    "# Load datasets\n",
    "potential_divcorr = pd.read_csv('potential_divcorr.csv')\n",
    "epsilon = pd.read_csv('epsilon.csv')\n",
    "simulcounter = pd.read_csv('simulcounter.csv')\n",
    "fundcounter = pd.read_csv('fundcounter.csv')\n",
    "\n",
    "# Filter the data and merge necessary information\n",
    "potential_divcorr = potential_divcorr[['companyid', 'mydate', 'id', 'ret', 'excess_ret', 'alpha']]\n",
    "potential_divcorr = potential_divcorr.sort_values(by=['id', 'mydate'])\n",
    "potential_divcorr = potential_divcorr.merge(epsilon, on=['id', 'mydate'], how='left')\n",
    "potential_divcorr = potential_divcorr.merge(simulcounter, on='companyid', how='inner')\n",
    "\n",
    "# Filter firms with at least two funds simultaneously\n",
    "potential_divcorr = potential_divcorr[potential_divcorr['simulcounter'] > 1]\n",
    "\n",
    "# Pivot data to wide format using actual IDs\n",
    "df_wide = potential_divcorr.pivot_table(index=[\"companyid\", \"mydate\"], columns=\"id\", values=[\"ret\", \"epsilon\"], aggfunc='first')\n",
    "\n",
    "# Flatten multi-index columns\n",
    "df_wide.columns = [f\"{col[0]}_{col[1]}\" for col in df_wide.columns]\n",
    "df_wide.reset_index(inplace=True)\n",
    "\n",
    "# Initialize the parameter `last`\n",
    "last = 65\n",
    "\n",
    "# Function to calculate and save correlations using actual IDs\n",
    "def calculate_and_save_correlations(df_wide, last):\n",
    "    results = []\n",
    "    columns = df_wide.columns\n",
    "    fund_ids = [col.split(\"_\")[1] for col in columns if \"ret_\" in col]\n",
    "    unique_fund_ids = sorted(set(fund_ids))\n",
    "    \n",
    "    for idx1 in range(len(unique_fund_ids)):\n",
    "        fund_id1 = unique_fund_ids[idx1]\n",
    "        for idx2 in range(idx1 + 1, len(unique_fund_ids)):\n",
    "            if idx2 - idx1 >= last:\n",
    "                break\n",
    "            fund_id2 = unique_fund_ids[idx2]\n",
    "            relevant_columns = [f\"ret_{fund_id1}\", f\"ret_{fund_id2}\", f\"epsilon_{fund_id1}\", f\"epsilon_{fund_id2}\"]\n",
    "            if all(col in df_wide.columns for col in relevant_columns):\n",
    "                temp_df = df_wide[['companyid', 'mydate'] + relevant_columns].dropna()\n",
    "                if len(temp_df) >= 12:\n",
    "                    correlations = temp_df.corr()\n",
    "                    ret_corr = correlations.loc[f\"ret_{fund_id1}\", f\"ret_{fund_id2}\"]\n",
    "                    epsilon_corr = correlations.loc[f\"epsilon_{fund_id1}\", f\"epsilon_{fund_id2}\"]\n",
    "                    results.append({\n",
    "                        \"companyid\": temp_df['companyid'].iloc[0],\n",
    "                        \"div_cons1\": fund_id1,\n",
    "                        \"div_cons2\": fund_id2,\n",
    "                        \"div_corr\": ret_corr,\n",
    "                        \"corr_eps\": epsilon_corr\n",
    "                    })\n",
    "                    # Save results to CSV, one file per pair\n",
    "                    result_df = pd.DataFrame([results[-1]])\n",
    "                    result_df.to_csv(f\"div_corr_{fund_id1}_{fund_id2}.csv\", index=False)\n",
    "\n",
    "# Calculate and save correlations\n",
    "calculate_and_save_correlations(df_wide, last)\n",
    "\n",
    "# Combine all correlation files into a single DataFrame\n",
    "complete_divcorr = pd.DataFrame()\n",
    "correlation_files = glob.glob(\"div_corr_*.csv\")\n",
    "for file in correlation_files:\n",
    "    temp_df = pd.read_csv(file)\n",
    "    complete_divcorr = pd.concat([complete_divcorr, temp_df], ignore_index=True)\n",
    "    complete_divcorr.sort_values(by=['companyid', 'div_cons1', 'div_cons2'], inplace=True)\n",
    "    complete_divcorr['div_corr_id'] = range(1, len(complete_divcorr)+1)\n",
    "complete_divcorr.to_csv(\"complete_divcorr.csv\", index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T17:01:48.053935300Z",
     "start_time": "2024-08-07T17:01:30.055313300Z"
    }
   },
   "id": "5ea412d96e56ea00",
   "execution_count": 139
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable. Did you mean: 'glob.glob(...)'?",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[140], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m n \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m----> 4\u001B[0m     os\u001B[38;5;241m.\u001B[39mremove(glob(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdiv_corr_*.csv\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m:\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: 'module' object is not callable. Did you mean: 'glob.glob(...)'?"
     ]
    }
   ],
   "source": [
    "for x in range(1, 100000):\n",
    "    n = x + 1\n",
    "    try:\n",
    "        os.remove(glob('div_corr_*.csv'))\n",
    "    except FileNotFoundError:\n",
    "        continue"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T17:01:48.111611700Z",
     "start_time": "2024-08-07T17:01:48.057690700Z"
    }
   },
   "id": "2391a814e511c740",
   "execution_count": 140
  },
  {
   "cell_type": "markdown",
   "source": [
    "RET_CORR_INTEGRATE -> zit al bij BIG "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbe2bcdbed64671c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Enig hier is dat ipv tm 65, de laatste 65 worden gepakt, nog niet helemaal duidelijk wat het uitmaakt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "231fb5954cc552fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "RET_CORR_INTEGRATE 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c53a99283643eb27"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load necessary datasets\n",
    "complete_divcorr = pd.read_csv('complete_divcorr.csv')\n",
    "potential_treat0 = pd.read_csv('potential_treat0.csv')\n",
    "\n",
    "# Step 1: Make a list of every fund (id) in complete_divcorr and call it potential_treat1\n",
    "divcorr_id1 = complete_divcorr[['div_cons1']].rename(columns={'div_cons1': 'id'})\n",
    "divcorr_id2 = complete_divcorr[['div_cons2']].rename(columns={'div_cons2': 'id'})\n",
    "potential_treat1 = pd.concat([divcorr_id1, divcorr_id2]).drop_duplicates().sort_values(by='id')\n",
    "potential_treat1.to_csv('potential_treat1.csv', index=False)\n",
    "\n",
    "# Step 2: Merge in everything from potential_treat0\n",
    "potential_treat2 = potential_treat0.merge(potential_treat1, on='id', how='left')\n",
    "potential_treat2['closedxcrisis'].fillna(0, inplace=True)\n",
    "potential_treat2['closedxcrisis'] = potential_treat2['closedxcrisis'].astype(int)\n",
    "potential_treat2['fund_closedxcrisis'] = potential_treat2.groupby('id')['closedxcrisis'].transform('max')\n",
    "potential_treat2.to_csv('potential_treat2.csv', index=False)\n",
    "\n",
    "# Step 3: Create a file of ids with min_mydate\n",
    "minmydate = potential_treat2[potential_treat2['mydate'] == potential_treat2['min_mydate']]\n",
    "minmydate = minmydate[['id', 'min_mydate']]\n",
    "minmydate.to_csv('minmydate.csv', index=False)\n",
    "\n",
    "oth_minmydate = minmydate.rename(columns={'id': 'other_fund', 'min_mydate': 'oth_minmydate'})\n",
    "oth_minmydate.to_csv('oth_minmydate.csv', index=False)\n",
    "\n",
    "# Step 4: Eliminate \"twins\"\n",
    "# Batch 1\n",
    "batch1 = complete_divcorr[['companyid', 'div_cons1', 'div_cons2', 'div_corr', 'corr_eps']]\n",
    "batch1 = batch1.rename(columns={'div_cons1': 'id', 'div_cons2': 'other_fund'})\n",
    "batch1.to_csv('batch1.csv', index=False)\n",
    "\n",
    "# Batch 2\n",
    "batch2 = complete_divcorr[['companyid', 'div_cons1', 'div_cons2', 'div_corr', 'corr_eps']]\n",
    "batch2 = batch2.rename(columns={'div_cons2': 'id', 'div_cons1': 'other_fund'})\n",
    "batch2 = pd.concat([batch2, batch1]).drop_duplicates().sort_values(by=['companyid', 'id', 'other_fund'])\n",
    "\n",
    "batch2 = batch2.merge(minmydate, on='id', how='inner').rename(columns={'min_mydate': 'id_minmydate'})\n",
    "batch2 = batch2.merge(oth_minmydate, on='other_fund', how='inner')\n",
    "batch2.to_csv('batch2.csv', index=False)\n",
    "\n",
    "# Batch 3: Focus on (unique) funds with div_corr > 0.985\n",
    "batch3 = batch2.copy()\n",
    "batch3['corr_eps'].replace(-999, np.nan, inplace=True)\n",
    "batch3 = batch3[batch3['div_corr'] > 0.985]\n",
    "batch3.drop_duplicates(subset=['id', 'other_fund'], inplace=True)\n",
    "\n",
    "batch3['keep_id'] = np.where(batch3['id_minmydate'] <= batch3['oth_minmydate'], 1, 0)\n",
    "batch3['keep_other_fund'] = np.where(batch3['oth_minmydate'] < batch3['id_minmydate'], 1, 0)\n",
    "batch3.to_csv('batch3.csv', index=False)\n",
    "\n",
    "# Batch 4: Identify other_fund ids when keep_id==0 for an id and rename other_fund id\n",
    "batch4 = batch3[batch3['keep_id'] == 0][['other_fund', 'div_corr']]\n",
    "batch4 = batch4.rename(columns={'other_fund': 'id'})\n",
    "batch4.to_csv('batch4.csv', index=False)\n",
    "\n",
    "# Highcorr_set: Set of funds to be kept, even though they are highly correlated\n",
    "highcorr_set = pd.concat([\n",
    "    batch3[batch3['keep_other_fund'] == 0][['id', 'div_corr']],\n",
    "    batch4\n",
    "]).drop_duplicates(subset=['id']).sort_values(by='id')\n",
    "highcorr_set.to_csv('highcorr_set.csv', index=False)\n",
    "\n",
    "# Dropcorr_set: Set of \"twin\" funds to be dropped\n",
    "dropcorr_set = batch3[['id']].drop_duplicates().sort_values(by='id')\n",
    "dropcorr_set = dropcorr_set.merge(highcorr_set, on='id', how='left', indicator=True)\n",
    "dropcorr_set = dropcorr_set[dropcorr_set['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "dropcorr_set.to_csv('dropcorr_set.csv', index=False)\n",
    "\n",
    "# Define the test set\n",
    "intheset = potential_treat2.copy()\n",
    "intheset = intheset.merge(dropcorr_set, on='id', how='left', indicator=True)\n",
    "intheset['intheset'] = 1\n",
    "intheset.loc[intheset['crisis'] == 1, 'intheset'] = 0\n",
    "intheset.loc[intheset['fund_closedxcrisis'] == 1, 'intheset'] = 0\n",
    "intheset.loc[intheset['min_mydate'] > 562, 'intheset'] = 0\n",
    "intheset.loc[intheset['_merge'] == 'both', 'intheset'] = 0\n",
    "intheset.drop(columns=['_merge'], inplace=True)\n",
    "intheset = intheset[intheset['intheset'] == 1]\n",
    "\n",
    "intheset['intheset2'] = np.where((intheset['mydate'] <= 625) & (intheset['mydate'] >= 617), 0, intheset['intheset'])\n",
    "intheset['counter'] = intheset.groupby('id').cumcount() + 1\n",
    "intheset['maxcounter'] = intheset.groupby('id')['counter'].transform('max')\n",
    "intheset['intheset'] = np.where(intheset['maxcounter'] < 12, 0, intheset['intheset'])\n",
    "\n",
    "intheset = intheset[intheset['intheset'] == 1][['id', 'mydate', 'intheset', 'intheset2']]\n",
    "intheset.to_csv('intheset.csv', index=False)\n",
    "\n",
    "# Bring in all pairwise correlations\n",
    "integrate2 = complete_divcorr[['companyid', 'div_cons1', 'div_cons2', 'div_corr', 'corr_eps']]\n",
    "integrate2 = integrate2.rename(columns={'div_cons2': 'id', 'div_cons1': 'other_fund'})\n",
    "integrate2 = pd.concat([integrate2, batch1]).drop_duplicates().sort_values(by=['companyid', 'id', 'other_fund'])\n",
    "integrate2['corr_eps'].replace(-999, np.nan, inplace=True)\n",
    "integrate2.to_csv('integrate2.csv', index=False)\n",
    "\n",
    "# Identify funds with div_corr that were closed during the crisis\n",
    "closed = potential_treat2[potential_treat2['closedxcrisis'] == 1][['id']]\n",
    "closed.to_csv('closed.csv', index=False)\n",
    "\n",
    "# Merge closed fund with list of funds w/div_corr to find the \"real\" treatments\n",
    "integrate3 = closed.merge(integrate2, on='id', how='inner')\n",
    "integrate3['treatment99'] = np.where(integrate3['div_corr'] < 0.985, 1, 0)\n",
    "integrate3['treatment90'] = np.where(integrate3['div_corr'] < 0.895, 1, 0)\n",
    "integrate3['treatment99_eps'] = np.where((integrate3['corr_eps'] < 0.985) & (integrate3['corr_eps'].notna()), 1, 0)\n",
    "integrate3['treatment90_eps'] = np.where((integrate3['corr_eps'] < 0.895) & (integrate3['corr_eps'].notna()), 1, 0)\n",
    "\n",
    "integrate3 = integrate3.rename(columns={'id': 'treated_id', 'other_fund': 'id'}).sort_values(by=['companyid', 'id'])\n",
    "integrate3.to_csv('integrate3.csv', index=False)\n",
    "\n",
    "# Merge with potential_treat2\n",
    "integrate3 = integrate3.merge(potential_treat2, on=['companyid', 'id'], how='outer')\n",
    "\n",
    "# DIV_CORR < 0.985\n",
    "integrate3['closedxcrisis2'] = integrate3['closedxcrisis']\n",
    "integrate3['closedxcrisis3'] = integrate3['closedxcrisis']\n",
    "integrate3['closedxcrisis4'] = integrate3['closedxcrisis']\n",
    "integrate3.loc[integrate3['div_corr'] >= 0.985, 'closedxcrisis'] = 0\n",
    "integrate3['fund_closedxcrisis'] = integrate3.groupby('id')['closedxcrisis'].transform('max')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-07T17:01:48.086946700Z"
    }
   },
   "id": "3c9f278779252fd1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Instantaneous and constant FIRM measures of closure during the crisis\n",
    "integrate3['firm_closedxcrisis'] = integrate3.groupby(['companyid', 'mydate'])['closedxcrisis'].transform('max')\n",
    "integrate3['max_firm_closedxcrisis'] = integrate3.groupby('companyid')['closedxcrisis'].transform('max')\n",
    "\n",
    "# A fund cannot be treated if it started within one year of the financial crisis\n",
    "integrate3.loc[integrate3['min_mydate'] > 562, 'max_firm_closedxcrisis'] = 0\n",
    "integrate3.loc[integrate3['div_corr'] >= 0.985, 'max_firm_closedxcrisis'] = 0\n",
    "\n",
    "# crisis_treat24 equals one for two years after the end of the crisis\n",
    "integrate3['crisis_treat24'] = 0\n",
    "integrate3.loc[(integrate3['max_firm_closedxcrisis'] == 1) & (integrate3['mydate'] >= 595) & (integrate3['mydate'] <= 619), 'crisis_treat24'] = 1\n",
    "\n",
    "# DIV_CORR < 0.895\n",
    "integrate3.loc[integrate3['div_corr'] >= 0.895, 'closedxcrisis2'] = 0\n",
    "integrate3['fund_closedxcrisis2'] = integrate3.groupby('id')['closedxcrisis2'].transform('max')\n",
    "\n",
    "# Instantaneous and constant FIRM measures of closure during the crisis\n",
    "integrate3['firm_closedxcrisis2'] = integrate3.groupby(['companyid', 'mydate'])['closedxcrisis2'].transform('max')\n",
    "integrate3['max_firm_closedxcrisis2'] = integrate3.groupby('companyid')['closedxcrisis2'].transform('max')\n",
    "\n",
    "# A fund cannot be treated if it started within one year of the financial crisis\n",
    "integrate3.loc[integrate3['min_mydate'] > 562, 'max_firm_closedxcrisis2'] = 0\n",
    "integrate3.loc[integrate3['div_corr'] >= 0.895, 'max_firm_closedxcrisis2'] = 0\n",
    "\n",
    "# crisis_treat24 equals one for two years after the end of the crisis\n",
    "integrate3['crisis_treat24_90'] = 0\n",
    "integrate3.loc[(integrate3['max_firm_closedxcrisis2'] == 1) & (integrate3['mydate'] >= 595) & (integrate3['mydate'] <= 619), 'crisis_treat24_90'] = 1\n",
    "\n",
    "# CORR_EPS < 0.985\n",
    "integrate3.loc[integrate3['corr_eps'] >= 0.985, 'closedxcrisis3'] = 0\n",
    "integrate3['fund_closedxcrisis3'] = integrate3.groupby('id')['closedxcrisis3'].transform('max')\n",
    "\n",
    "# Instantaneous and constant FIRM measures of closure during the crisis\n",
    "integrate3['firm_closedxcrisis3'] = integrate3.groupby(['companyid', 'mydate'])['closedxcrisis3'].transform('max')\n",
    "integrate3['max_firm_closedxcrisis3'] = integrate3.groupby('companyid')['closedxcrisis3'].transform('max')\n",
    "\n",
    "# A fund cannot be treated if it started within one year of the financial crisis\n",
    "integrate3.loc[integrate3['min_mydate'] > 562, 'max_firm_closedxcrisis3'] = 0\n",
    "integrate3.loc[integrate3['div_corr'] >= 0.985, 'max_firm_closedxcrisis3'] = 0\n",
    "\n",
    "# crisis_treat24 equals one for two years after the end of the crisis\n",
    "integrate3['crisis_treat24_99eps'] = 0\n",
    "integrate3.loc[(integrate3['max_firm_closedxcrisis3'] == 1) & (integrate3['mydate'] >= 595) & (integrate3['mydate'] <= 619), 'crisis_treat24_99eps'] = 1\n",
    "\n",
    "# CORR_EPS < 0.895\n",
    "integrate3.loc[integrate3['corr_eps'] >= 0.895, 'closedxcrisis4'] = 0\n",
    "integrate3['fund_closedxcrisis4'] = integrate3.groupby('id')['closedxcrisis4'].transform('max')\n",
    "\n",
    "# Instantaneous and constant FIRM measures of closure during the crisis\n",
    "integrate3['firm_closedxcrisis4'] = integrate3.groupby(['companyid', 'mydate'])['closedxcrisis4'].transform('max')\n",
    "integrate3['max_firm_closedxcrisis4'] = integrate3.groupby('companyid')['closedxcrisis4'].transform('max')\n",
    "\n",
    "# A fund cannot be treated if it started within one year of the financial crisis\n",
    "integrate3.loc[integrate3['min_mydate'] > 562, 'max_firm_closedxcrisis4'] = 0\n",
    "integrate3.loc[integrate3['div_corr'] >= 0.895, 'max_firm_closedxcrisis4'] = 0\n",
    "\n",
    "# crisis_treat24 equals one for two years after the end of the crisis\n",
    "integrate3['crisis_treat24_90eps'] = 0\n",
    "integrate3.loc[(integrate3['max_firm_closedxcrisis4'] == 1) & (integrate3['mydate'] >= 595) & (integrate3['mydate'] <= 619), 'crisis_treat24_90eps'] = 1\n",
    "\n",
    "integrate3.to_csv('integrate4.csv', index=False)\n",
    "\n",
    "# Clean up\n",
    "import os\n",
    "files_to_erase = [\n",
    "    'batch1.csv', 'batch3.csv', 'batch4.csv', 'potential_treat1.csv', \n",
    "    'integrate2.csv', 'highcorr_set.csv'\n",
    "]\n",
    "\n",
    "for file in files_to_erase:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "\n",
    "print(\"Integration completed and temporary files cleaned up.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-07T17:01:48.090254500Z"
    }
   },
   "id": "e5771ae3a8376d14",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-07T17:01:48.094282300Z"
    }
   },
   "id": "515bac032aacf35"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8333ef7e2d0775d2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
