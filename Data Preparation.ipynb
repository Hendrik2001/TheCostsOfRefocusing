{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "MAKEHF1DATA"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ac9823ff687378d"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import openpyxl\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T11:01:16.887408Z",
     "start_time": "2024-06-28T11:01:16.724225Z"
    }
   },
   "id": "53d6d73c8cfc5a44"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-28T12:17:53.374939Z",
     "start_time": "2024-06-28T12:17:53.260126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id        date       ret        aum  companyid         Main Strategy  \\\n",
      "0  862  31/01/2002  0,008599  975684231         66  European Long Biased   \n",
      "1  862  31/03/2003  0,011499  873171102         66  European Long Biased   \n",
      "2  862  30/06/2003  0,018399  746943753         66  European Long Biased   \n",
      "3  862  31/05/2003  0,014799  742376533         66  European Long Biased   \n",
      "4  862  29/02/2004  0,018899  822674719         66  European Long Biased   \n",
      "\n",
      "  incentivefee managementfee  \n",
      "0          0,2          0,01  \n",
      "1          0,2          0,01  \n",
      "2          0,2          0,01  \n",
      "3          0,2          0,01  \n",
      "4          0,2          0,01  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load the merged data\n",
    "merged_df = (pd.read_csv('Partial.csv', sep=';', low_memory=False))\n",
    "print(merged_df.head())\n",
    "# Load the mydate converter data\n",
    "mydate_converter = pd.read_csv('mydate_converter.csv', names=['year', 'month', 'mydate'], skiprows=[0])\n",
    "\n",
    "\n",
    "# Convert 'date' to datetime format and extract year and month\n",
    "merged_df['date'] = pd.to_datetime(merged_df['date'], format='%d/%m/%Y', errors='coerce')\n",
    "merged_df['year'] = merged_df['date'].dt.year\n",
    "merged_df['month'] = merged_df['date'].dt.month\n",
    "\n",
    "# Ensure the 'aum' and 'ret' columns are properly formatted\n",
    "merged_df['aum'] = merged_df['aum'].astype(str).str.replace(',', '.').astype(float)\n",
    "merged_df['ret'] = merged_df['ret'].astype(str).str.replace(',', '.').astype(float)\n",
    "# Merge with mydate_converter\n",
    "mydate_converter = mydate_converter[pd.to_numeric(mydate_converter['year'], errors='coerce').notnull()]\n",
    "mydate_converter['year'] = mydate_converter['year'].astype(int)\n",
    "mydate_converter['month'] = mydate_converter['month'].astype(int)\n",
    "mydate_converter['mydate'] = mydate_converter['mydate'].astype(int)\n",
    "\n",
    "# Merge with mydate_converter\n",
    "merged_df = merged_df.merge(mydate_converter, on=['year', 'month'], how='left')\n",
    "\n",
    "# Step 2: Filter and Clean the Data\n",
    "# Keep records from 1994 onwards\n",
    "merged_df = merged_df[merged_df['year'] >= 1994]\n",
    "\n",
    "# Generate elapsed time\n",
    "merged_df['maxmydate'] = pd.to_numeric(merged_df.groupby('id')['mydate'].transform('max'), errors='coerce')\n",
    "merged_df['minmydate'] = pd.to_numeric(merged_df.groupby('id')['mydate'].transform('min'), errors='coerce')\n",
    "merged_df['elapsedtime'] = merged_df['maxmydate'] - merged_df['minmydate'] + 1\n",
    "\n",
    "# Filter out invalid records\n",
    "merged_df = merged_df[(merged_df['aum'] <= 100000000000) & (merged_df['aum'] >= 1000000)]\n",
    "merged_df = merged_df[merged_df['ret'] <= 1000]\n",
    "\n",
    "# Identify sporadic reporters\n",
    "retcounter = merged_df.groupby('id').size().reset_index(name='ret_counter')\n",
    "\n",
    "merged_df = merged_df.merge(retcounter, on='id', how='left')\n",
    "merged_df['sporadic_dum'] = (merged_df['elapsedtime'] > merged_df['ret_counter']).astype(int)\n",
    "max_sporadic = merged_df.groupby('id')['sporadic_dum'].transform('max')\n",
    "merged_df = merged_df[max_sporadic == 0]\n",
    "# Drop funds with fewer than 12 months of data\n",
    "merged_df = merged_df[merged_df['ret_counter'] >= 12]\n",
    "merged_df = merged_df[merged_df['ret'].notna()]\n",
    "\n",
    "# Drop unnecessary columns\n",
    "merged_df.drop(columns=['elapsedtime', 'ret_counter'], inplace=True)\n",
    "\n",
    "# Save the cleaned data\n",
    "merged_df.to_csv('tass2.csv', index=False)\n",
    "\n",
    "# Step 3: Break Data into Pre-Crisis, Crisis, and Post-Crisis Periods\n",
    "# Split data into pre-crisis, crisis, and post-crisis periods\n",
    "tass_pre = merged_df[merged_df['mydate'] < 574]\n",
    "tass_pre.to_csv('tass2_pre.csv', index=False)\n",
    "\n",
    "tass_crisis = merged_df[(merged_df['mydate'] >= 574) & (merged_df['mydate'] <= 593)]\n",
    "tass_crisis.to_csv('tass2_crisis.csv', index=False)\n",
    "\n",
    "tass_post = merged_df[merged_df['mydate'] > 593]\n",
    "tass_post.to_csv('tass2_post.csv', index=False)\n",
    "\n",
    "# Step 4: Perform AR1 Adjustment\n",
    "def ar1_adjustment(df, period_name):\n",
    "    df = df.sort_values(by=['id', 'mydate'])\n",
    "    df['rho'] = 0.0\n",
    "    unique_ids = df['id'].unique()\n",
    "\n",
    "    for unique_id in unique_ids:\n",
    "        sub_df = df[df['id'] == unique_id]\n",
    "        if len(sub_df) > 1:\n",
    "            model = sm.OLS(sub_df['ret'].iloc[1:], sm.add_constant(sub_df['ret'].shift(1).iloc[1:])).fit()\n",
    "            rho = model.params.iloc[1] if len(model.params) > 1 else 0  # Use iloc for positional access\n",
    "            df.loc[df['id'] == unique_id, 'rho'] = rho\n",
    "\n",
    "    df['ret_star'] = (df['ret'] - df['rho'] * df['ret'].shift(1)) / (1 - df['rho'])\n",
    "    df.to_csv(f'tass4_{period_name}.csv', index=False)\n",
    "    return df\n",
    "\n",
    "# AR1 adjustment for pre-crisis period\n",
    "tass_pre_adjusted = ar1_adjustment(tass_pre, 'pre')\n",
    "\n",
    "# AR1 adjustment for crisis period\n",
    "tass_crisis_adjusted = ar1_adjustment(tass_crisis, 'crisis')\n",
    "\n",
    "# AR1 adjustment for post-crisis period\n",
    "tass_post_adjusted = ar1_adjustment(tass_post, 'post')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year  month     DGS10  mydate\n",
      "0  1993      1  6.600000     396\n",
      "1  1993      2  6.258947     397\n",
      "2  1993      3  5.975217     398\n",
      "3  1993      4  5.969524     399\n",
      "4  1993      5  6.035500     400\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def load_and_clean_data_with_mydate(file_path, mydate_converter, keep_columns=None, drop_na_columns=None):\n",
    "    file_ext = file_path.split('.')[-1]\n",
    "    if file_ext == 'csv':\n",
    "        df = pd.read_csv(file_path)\n",
    "    elif file_ext in ['xls', 'xlsx']:\n",
    "        df = pd.read_excel(file_path)\n",
    "\n",
    "    # Merge with mydate_converter\n",
    "    mydate_converter = mydate_converter[pd.to_numeric(mydate_converter['year'], errors='coerce').notnull()]\n",
    "\n",
    "    mydate_converter['year'] = mydate_converter['year'].astype(int)\n",
    "    mydate_converter['month'] = mydate_converter['month'].astype(int)\n",
    "    mydate_converter['mydate'] = mydate_converter['mydate'].astype(int)\n",
    "\n",
    "    df = df.merge(mydate_converter, on=['year', 'month'], how='left')\n",
    "    \n",
    "    if drop_na_columns:\n",
    "        df = df.dropna(subset=drop_na_columns)\n",
    "    if keep_columns:\n",
    "        df = df[keep_columns]\n",
    "    \n",
    "    df = df.sort_values(by=['year', 'month'])\n",
    "    return df\n",
    "\n",
    "# Load the mydate converter data\n",
    "mydate_converter = pd.read_csv('mydate_converter.csv', names=['year', 'month', 'mydate'], skiprows=[0])\n",
    "\n",
    "# Load factor files with mydate\n",
    "df_ff = load_and_clean_data_with_mydate('Factors/Corrected_FF_Research_Data_Factors.csv', mydate_converter, drop_na_columns=['month'])\n",
    "df_fung_hsieh = load_and_clean_data_with_mydate('Factors/TF-Fac.xlsx', mydate_converter, keep_columns=['PTFSBD', 'PTFSFX', 'PTFSCOM', 'year', 'month'], drop_na_columns=['year'])\n",
    "df_mom = load_and_clean_data_with_mydate('Factors/Corrected_FF_Momentum_Factor.csv', mydate_converter)\n",
    "df_bond = load_and_clean_data_with_mydate('Factors/DBAA_Monthly_Averages.csv', mydate_converter)\n",
    "df_credit = load_and_clean_data_with_mydate('Factors/DGS10_Monthly_Averages.csv', mydate_converter, drop_na_columns=['year'])\n",
    "print(df_credit.head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T12:17:55.791587Z",
     "start_time": "2024-06-28T12:17:55.734979Z"
    }
   },
   "id": "8292d8526a46cf22",
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to perform AR1 adjustment\n",
    "def ar1_adjustment(df, period_name):\n",
    "    df = df.copy()\n",
    "    df['ret_star'] = df['ret']\n",
    "    unique_ids = df['id'].unique()\n",
    "    for unique_id in unique_ids:\n",
    "        subset = df[df['id'] == unique_id]\n",
    "        if len(subset) > 1:\n",
    "            subset = subset.sort_values(by='mydate')\n",
    "            X = sm.add_constant(subset['ret'].shift(1).dropna())\n",
    "            y = subset['ret'].iloc[1:]\n",
    "            if len(X) == len(y):  # Ensure X and y have the same length\n",
    "                try:\n",
    "                    model = sm.OLS(y, X).fit()\n",
    "                    rho = model.params.iloc[1] if len(model.params) > 1 else 0  # Default to 0 if model fitting fails\n",
    "                    df.loc[df['id'] == unique_id, 'ret_star'] = (df['ret'] - rho * df['ret'].shift(1)) / (1 - rho)\n",
    "                except Exception as e:\n",
    "                    print(f\"Model fitting failed for id {unique_id} with error: {e}\")\n",
    "    df.to_csv(f'tass4_{period_name}.csv', index=False)\n",
    "    return df\n",
    "\n",
    "# Load merged data\n",
    "\n",
    "merged_df['mydate'] = merged_df['mydate'].astype(int)\n",
    "\n",
    "# Define periods\n",
    "pre_crisis_period = merged_df[(merged_df['mydate'] >= 0) & (merged_df['mydate'] < 575)]\n",
    "crisis_period = merged_df[(merged_df['mydate'] >= 575) & (merged_df['mydate'] <= 593)]\n",
    "post_crisis_period = merged_df[(merged_df['mydate'] > 593)]\n",
    "\n",
    "# Perform AR1 adjustment\n",
    "tass4_pre = ar1_adjustment(pre_crisis_period, 'pre')\n",
    "tass4_crisis = ar1_adjustment(crisis_period, 'crisis')\n",
    "tass4_post = ar1_adjustment(post_crisis_period, 'post')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T12:17:57.436835Z",
     "start_time": "2024-06-28T12:17:57.358459Z"
    }
   },
   "id": "7c101f2e181229ba",
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to merge factors with TASS data\n",
    "def merge_factors(df_tass, factors_list):\n",
    "    for factor_df in factors_list:\n",
    "        df_tass = pd.merge(df_tass, factor_df, on=['year', 'month'], how='left', indicator=True, suffixes=('','_remove'))\n",
    "        df_tass.drop([i for i in df_tass.columns if 'remove' in i], axis=1, inplace=True)\n",
    "        df_tass = df_tass[df_tass['_merge'] == 'both'].drop('_merge', axis=1)\n",
    "    return df_tass\n",
    "\n",
    "# Load the adjusted TASS data for each period\n",
    "df_tass4_pre = pd.read_csv('tass4_pre.csv')\n",
    "df_tass4_crisis = pd.read_csv('tass4_crisis.csv')\n",
    "df_tass4_post = pd.read_csv('tass4_post.csv')\n",
    "\n",
    "# List of factor dataframes\n",
    "factors_list = [df_ff, df_fung_hsieh, df_mom, df_bond, df_credit]\n",
    "\n",
    "# Merging factors with TASS data for each period\n",
    "df_tass4_pre = merge_factors(df_tass4_pre, factors_list)\n",
    "df_tass4_crisis = merge_factors(df_tass4_crisis, factors_list)\n",
    "df_tass4_post = merge_factors(df_tass4_post, factors_list)\n",
    "\n",
    "# Save the merged dataframes to CSV for further use\n",
    "df_tass4_pre.to_csv('tass4_pre_merged.csv', index=False)\n",
    "df_tass4_crisis.to_csv('tass4_crisis_merged.csv', index=False)\n",
    "df_tass4_post.to_csv('tass4_post_merged.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T12:17:58.162752Z",
     "start_time": "2024-06-28T12:17:58.112335Z"
    }
   },
   "id": "253f360806dc1828",
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jy/cfb14z9n637_y0c4jp4g2j1h0000gn/T/ipykernel_29273/118254282.py:32: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[10.70340314 10.69702528 10.72526728 10.69119425 10.68186826 10.7402783\n",
      " 10.70420218 10.71940932 10.67680264 10.71305962 10.69475849 10.72564189\n",
      " 10.71952427 10.71163142 10.72665276 10.74352887 10.68404206 10.72918996\n",
      " 10.72366948 10.73380601 10.7222289  10.71132387 10.72392787 10.7078442 ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[df['id'] == unique_id, 'excess_ret'] = df['lhs'] - predictions + df['alpha']\n"
     ]
    }
   ],
   "source": [
    "def asset_pricing(df, period_name):\n",
    "    df['lhs'] = df['ret_star'] - df['RF']\n",
    "    df['excess_ret'] = 0\n",
    "    df['beta1'] = 0.0\n",
    "    df['beta2'] = 0.0\n",
    "    df['beta3'] = 0.0\n",
    "    df['beta4'] = 0.0\n",
    "    df['beta5'] = 0.0\n",
    "    df['beta6'] = 0.0\n",
    "    df['beta7'] = 0.0\n",
    "    df['alpha'] = 0.0\n",
    "    df['stdv'] = 0.0\n",
    "    df['r2'] = 0.0\n",
    "\n",
    "    df.rename(columns={'mktrf': 'eq_prem'}, inplace=True)\n",
    "    unique_ids = df['id'].unique()\n",
    "\n",
    "    for unique_id in unique_ids:\n",
    "        sub_df = df[df['id'] == unique_id]\n",
    "        if len(sub_df) > 1:\n",
    "            model = sm.OLS(sub_df['lhs'], sm.add_constant(sub_df[['Mkt-RF', 'SMB', 'PTFSBD', 'PTFSFX', 'PTFSCOM', 'year', 'DBAA']])).fit()\n",
    "            df.loc[df['id'] == unique_id, 'r2'] = model.rsquared\n",
    "            predictions = model.predict(sm.add_constant(sub_df[['Mkt-RF', 'SMB', 'PTFSBD', 'PTFSFX', 'PTFSCOM', 'year', 'DBAA']]))\n",
    "            if len(model.params) > 1: df.loc[df['id'] == unique_id, 'beta1'] = model.params.iloc[1]\n",
    "            if len(model.params) > 2: df.loc[df['id'] == unique_id, 'beta2'] = model.params.iloc[2]\n",
    "            if len(model.params) > 3: df.loc[df['id'] == unique_id, 'beta3'] = model.params.iloc[3]\n",
    "            if len(model.params) > 4: df.loc[df['id'] == unique_id, 'beta4'] = model.params.iloc[4]\n",
    "            if len(model.params) > 5: df.loc[df['id'] == unique_id, 'beta5'] = model.params.iloc[5]\n",
    "            if len(model.params) > 6: df.loc[df['id'] == unique_id, 'beta6'] = model.params.iloc[6]\n",
    "            if len(model.params) > 7: df.loc[df['id'] == unique_id, 'beta7'] = model.params.iloc[7]\n",
    "            if len(model.params) > 0: df.loc[df['id'] == unique_id, 'alpha'] = model.params.iloc[0]\n",
    "            df.loc[df['id'] == unique_id, 'excess_ret'] = df['lhs'] - predictions + df['alpha']\n",
    "            df.loc[df['id'] == unique_id, 'stdv'] = df['excess_ret'].std()\n",
    "\n",
    "    df['excess_ret'] = df['excess_ret'].where(df['ret'].notna())\n",
    "    df['excess_ret'] = df['excess_ret'].where(df['alpha'].notna())\n",
    "    df.to_csv(f'tass5_{period_name}.csv', index=False)\n",
    "    return df\n",
    "\n",
    "# Perform asset pricing analysis for each period\n",
    "tass5_pre = asset_pricing(df_tass4_pre, 'pre')\n",
    "tass5_crisis = asset_pricing(df_tass4_crisis, 'crisis')\n",
    "tass5_post = asset_pricing(df_tass4_post, 'post')\n",
    "\n",
    "# Combine all periods into a single dataset\n",
    "tass5 = pd.concat([tass5_pre, tass5_crisis, tass5_post])\n",
    "tass5.to_csv('tass5.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T12:17:58.880030Z",
     "start_time": "2024-06-28T12:17:58.680310Z"
    }
   },
   "id": "2a4279672ff5af0c",
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Preliminarily define \"closed\" funds during the crisis\n",
    "tass5 = pd.concat([tass5_pre, tass5_crisis, tass5_post])\n",
    "\n",
    "# Define the crisis and post periods\n",
    "tass5['crisis'] = 0\n",
    "tass5.loc[(tass5['mydate'] >= 573) & (tass5['mydate'] <= 594), 'crisis'] = 1\n",
    "\n",
    "tass5['post'] = 0\n",
    "tass5.loc[tass5['mydate'] > 594, 'post'] = 1\n",
    "\n",
    "# Identify the max and min mydate for each fund\n",
    "tass5['max_mydate'] = tass5.groupby('id')['mydate'].transform('max')\n",
    "tass5['min_mydate'] = tass5.groupby('id')['mydate'].transform('min')\n",
    "\n",
    "# Define funds that closed during the crisis\n",
    "tass5['closedxcrisis'] = 0\n",
    "tass5.loc[(tass5['max_mydate'] >= 573) & (tass5['max_mydate'] <= 594) & (tass5['mydate'] == tass5['max_mydate']), 'closedxcrisis'] = 1\n",
    "\n",
    "# Define firms that closed at least one fund during the crisis\n",
    "tass5['firm_closedxcrisis'] = tass5.groupby('companyid')['closedxcrisis'].transform('max')\n",
    "\n",
    "# Save the potential treatment dataset\n",
    "tass5.to_csv('potential_treat0.csv', index=False)\n",
    "\n",
    "# Create a dataset of firms that closed at least one fund during the crisis\n",
    "potential_divcorr = tass5[tass5['firm_closedxcrisis'] == 1].drop(columns=['firm_closedxcrisis', 'closedxcrisis'])\n",
    "potential_divcorr.to_csv('potential_divcorr.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T12:17:59.664711Z",
     "start_time": "2024-06-28T12:17:59.620575Z"
    }
   },
   "id": "2b81de57e704d968",
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Pre-define treatment\n",
    "diag1 = tass5.groupby('companyid').agg({'firm_closedxcrisis': 'max', 'mydate': 'max'}).reset_index()\n",
    "diag1.rename(columns={'firm_closedxcrisis': 'treated'}, inplace=True)\n",
    "diag1['firm_closed'] = 0\n",
    "diag1.loc[(diag1['mydate'] >= 573) & (diag1['mydate'] <= 593), 'firm_closed'] = 1\n",
    "diag1.to_csv('diag1.csv', index=False)\n",
    "\n",
    "diag2 = tass5.merge(diag1, on='companyid', how='left')\n",
    "diag2['mydate'] = diag2['mydate_x']\n",
    "diag2['mydate2'] = diag2['mydate']\n",
    "diag2 = diag2.groupby('id').agg({'firm_closed': 'max', 'closedxcrisis': 'max', 'treated': 'max', 'mydate': 'max', 'mydate2': 'min'}).reset_index()\n",
    "diag2['pre_treat'] = 0\n",
    "diag2.loc[(diag2['firm_closed'] == 0) & (diag2['treated'] == 1) & (diag2['closedxcrisis'] == 0) & (diag2['mydate2'] >= 562) & (diag2['mydate'] >= 605), 'pre_treat'] = 1\n",
    "diag2 = diag2[['id', 'pre_treat']]\n",
    "diag2.to_csv('diag2.csv', index=False)\n",
    "\n",
    "potential_treat0 = pd.read_csv('potential_treat0.csv')\n",
    "diag2 = pd.read_csv('diag2.csv')\n",
    "\n",
    "potential_treat0 = potential_treat0.merge(diag2, on='id', how='left').fillna(0)\n",
    "potential_treat0['pre_treat'] = potential_treat0['pre_treat'].astype(int)\n",
    "diag3 = potential_treat0.groupby('companyid').agg({'pre_treat': 'max'}).reset_index()\n",
    "diag3.to_csv('diag3.csv', index=False)\n",
    "\n",
    "# Clean up intermediate files\n",
    "import os\n",
    "\n",
    "files_to_delete = [\n",
    "    'dataff_fffactors.csv', 'dataff_fung_hsieh.csv', 'dataff_mom.csv', \n",
    "    'dataff_bond.csv', 'dataff_credit.csv', 'tass4_pre.csv', \n",
    "    'tass4_crisis.csv', 'tass4_post.csv', 'tass5_pre.csv', \n",
    "    'tass5_crisis.csv', 'tass5_post.csv'\n",
    "]\n",
    "\n",
    "for file in files_to_delete:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T12:18:00.230485Z",
     "start_time": "2024-06-28T12:18:00.202392Z"
    }
   },
   "id": "c383f3d88e8c307d",
   "execution_count": 63
  },
  {
   "cell_type": "markdown",
   "source": [
    "CAR36"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ddcb8b07e6a1991"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jy/cfb14z9n637_y0c4jp4g2j1h0000gn/T/ipykernel_29273/1806176948.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['sum_sq_ex'] = df.groupby('id').apply(sum_sq_ex).reset_index(level=0, drop=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('tass5.csv')\n",
    "\n",
    "# Ensure the dataframe is sorted by 'id' and 'mydate'\n",
    "df = df.sort_values(by=['id', 'mydate'])\n",
    "\n",
    "# Calculate the moving average of excess returns over the past 12 months\n",
    "df['moveave_ex'] = df.groupby('id')['excess_ret'].transform(lambda x: x.rolling(window=12, min_periods=1).mean())\n",
    "\n",
    "# Calculate the sum of squared deviations from the moving average\n",
    "def sum_sq_ex(group):\n",
    "    return ((group['excess_ret'] - group['moveave_ex'])**2).rolling(window=12, min_periods=1).sum()\n",
    "\n",
    "df['sum_sq_ex'] = df.groupby('id').apply(sum_sq_ex).reset_index(level=0, drop=True)\n",
    "\n",
    "# Calculate the variance of the moving average and the standard deviation\n",
    "df['var_ex_move'] = df['sum_sq_ex'] / 12\n",
    "df['stdv_ex_move'] = np.sqrt(df['var_ex_move'])\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['sum_sq_ex', 'var_ex_move'])\n",
    "\n",
    "# Generate a counter for each id\n",
    "df['counter'] = df.groupby('id').cumcount() + 1\n",
    "\n",
    "# Generate the retdum column\n",
    "df['retdum'] = np.where(df['excess_ret'].notna(), 1, 0)\n",
    "\n",
    "# Calculate retcounter for various lengths up to 36\n",
    "for i in range(1, 37):\n",
    "    df[f'retcounter{i}'] = df.groupby('id')['retdum'].transform(lambda x: x.shift(i).rolling(window=i, min_periods=1).sum())\n",
    "\n",
    "# Determine the maximum retcounter value for each row\n",
    "df['retcounter'] = df[[f'retcounter{i}' for i in range(1, 37)]].bfill(axis=1).iloc[:, 0]\n",
    "\n",
    "# Calculate the CAR values for various lengths up to 36\n",
    "for i in range(1, 37):\n",
    "    df[f'CAR{i}'] = df.groupby('id')['excess_ret'].transform(lambda x: x.shift(i).rolling(window=i, min_periods=1).sum())\n",
    "\n",
    "# Determine the appropriate CAR value based on retcounter\n",
    "df['CARstar'] = np.nan\n",
    "for i in range(1, 37):\n",
    "    df['CARstar'] = np.where(df['retcounter'] >= i, df[f'CAR{i}'], df['CARstar'])\n",
    "\n",
    "# Adjust retcounter values greater than 36 to 36\n",
    "df['retcounter'] = np.where(df['retcounter'] > 36, 36, df['retcounter'])\n",
    "\n",
    "# Calculate the average CAR over the period\n",
    "df['avgCAR36'] = df['CARstar'] / df['retcounter']\n",
    "\n",
    "# Save the intermediate result to a CSV file\n",
    "df.to_csv('car36a.csv', index=False)\n",
    "\n",
    "# Filter out rows where excess is missing\n",
    "df = df[df['excess_ret'].notna()]\n",
    "\n",
    "# Keep only necessary columns\n",
    "df = df[['id', 'mydate', 'avgCAR36', 'stdv_ex_move']]\n",
    "\n",
    "# Save the final result to a CSV file\n",
    "df.to_csv('car36.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T12:18:01.243680Z",
     "start_time": "2024-06-28T12:18:01.048599Z"
    }
   },
   "id": "b48281ac24d9b559",
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T12:18:01.400577Z",
     "start_time": "2024-06-28T12:18:01.395249Z"
    }
   },
   "id": "79f8dc71f5d8a001",
   "execution_count": 64
  },
  {
   "cell_type": "markdown",
   "source": [
    "ALL_CLOSURES"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b8c779fd3864bc7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the mydate converter\n",
    "mydate_converter = pd.read_csv('mydate_converter.csv', header=None, names=['year', 'month', 'mydate'], skiprows=[0])\n",
    "\n",
    "# Load the tass5 dataset\n",
    "tass5 = pd.read_csv('tass5.csv')\n",
    "\n",
    "# Calculate min and max mydate for each fund\n",
    "tass5['min_mydate'] = tass5.groupby('id')['mydate'].transform('min')\n",
    "tass5['max_mydate'] = tass5.groupby('id')['mydate'].transform('max')\n",
    "\n",
    "# Generate the starts data\n",
    "starts = tass5[['id', 'min_mydate']].drop_duplicates()\n",
    "starts = starts.groupby('min_mydate').size().reset_index(name='starts')\n",
    "starts.rename(columns={'min_mydate': 'mydate'}, inplace=True)\n",
    "\n",
    "# Generate the closures data\n",
    "closures = tass5[['id', 'max_mydate']].drop_duplicates()\n",
    "closures = closures.groupby('max_mydate').size().reset_index(name='closures')\n",
    "closures.rename(columns={'max_mydate': 'mydate'}, inplace=True)\n",
    "\n",
    "# Merge starts and closures data\n",
    "open_close = pd.merge(starts, closures, on='mydate', how='outer').fillna(0)\n",
    "\n",
    "# Merge with mydate converter\n",
    "open_close = pd.merge(open_close, mydate_converter, on='mydate', how='left')\n",
    "\n",
    "# Save the open_close data to Excel\n",
    "open_close.to_excel('open_close_TASS2015.xlsx', index=False)\n",
    "\n",
    "# Aggregate data by year\n",
    "open_close_year = open_close.groupby('year').sum().reset_index()\n",
    "open_close_year = open_close_year[['year', 'starts', 'closures']]\n",
    "\n",
    "# Save the open_close_year data to Excel\n",
    "open_close_year.to_excel('open_close_TASS2015_year.xlsx', index=False)\n",
    "\n",
    "# Generate the all_close_treat data\n",
    "tass5 = tass5[['id', 'companyid', 'mydate', 'min_mydate', 'max_mydate']].drop_duplicates()\n",
    "tass5['closed'] = np.where((tass5['mydate'] == tass5['max_mydate']) & (tass5['max_mydate'] <= 653), 1, 0)\n",
    "tass5['opened'] = np.where((tass5['mydate'] == tass5['min_mydate']) & (tass5['min_mydate'] >= 409), 1, 0)\n",
    "\n",
    "# Collapse data by companyid and mydate\n",
    "collapsed_data = tass5.groupby(['companyid', 'mydate']).sum().reset_index()\n",
    "\n",
    "# Create lagged closed variables\n",
    "for i in range(1, 31):\n",
    "    collapsed_data[f'L{i}closed'] = collapsed_data.groupby('companyid')['closed'].shift(i).fillna(0)\n",
    "\n",
    "# Calculate net openings and closings\n",
    "collapsed_data['net'] = collapsed_data['opened'] - collapsed_data[[f'L{i}closed' for i in range(1, 13)]].sum(axis=1)\n",
    "\n",
    "# Create open_close and all_close_treat variables\n",
    "collapsed_data['open_close'] = np.where((collapsed_data['opened'] == 1) & (collapsed_data['net'] <= 0), 1, 0)\n",
    "collapsed_data['all_close_treat'] = collapsed_data[[f'L{i}closed' for i in range(1, 31)]].max(axis=1)\n",
    "collapsed_data['time_treat_all'] = np.argmax(collapsed_data[[f'L{i}closed' for i in range(1, 31)]].values >= 1, axis=1) + 1\n",
    "collapsed_data['time_treat_all'] = np.where(collapsed_data['all_close_treat'] >= 1, collapsed_data['time_treat_all'], 0)\n",
    "\n",
    "# Keep necessary columns and sort\n",
    "collapsed_data = collapsed_data[['companyid', 'mydate', 'all_close_treat', 'time_treat_all']].sort_values(by=['companyid', 'mydate'])\n",
    "\n",
    "# Save the all_close_treat data\n",
    "collapsed_data.to_csv('all_close_treat.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T12:18:02.168040Z",
     "start_time": "2024-06-28T12:18:02.117632Z"
    }
   },
   "id": "3aba75e1e07f76c6",
   "execution_count": 65
  },
  {
   "cell_type": "markdown",
   "source": [
    "RET_CORR_PAIRS"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "775a2e45520f8dfa"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the potential_divcorr dataset\n",
    "potential_divcorr = pd.read_csv('potential_divcorr.csv')\n",
    "\n",
    "# Step 1: Count the number of funds per firm at any given point in time (simulcounter)\n",
    "potential_divcorr['simulcounter'] = potential_divcorr.groupby(['companyid', 'mydate']).cumcount() + 1\n",
    "potential_divcorr['maxsimulcounter'] = potential_divcorr.groupby('companyid')['simulcounter'].transform('max')\n",
    "simulcounter_df = potential_divcorr.drop_duplicates(subset=['companyid']).copy()\n",
    "simulcounter_df = simulcounter_df[['companyid', 'maxsimulcounter']].rename(columns={'maxsimulcounter': 'simulcounter'})\n",
    "# Save simulcounter data\n",
    "simulcounter_df.to_csv('simulcounter.csv', index=False)\n",
    "# Step 2: Count the number of funds per firm and filter firms with at least two funds simultaneously\n",
    "potential_divcorr = pd.merge(potential_divcorr, simulcounter_df, on='companyid', how='inner')\n",
    "potential_divcorr['simulcounter'] = potential_divcorr['simulcounter_y']\n",
    "potential_divcorr = potential_divcorr[potential_divcorr['simulcounter'] > 1]\n",
    "\n",
    "# Step 3: Create a sequential fund identifier within the firm (fundcounter)\n",
    "fundcounter_df = potential_divcorr.groupby('id').agg({'companyid': 'max', 'mydate': 'min'}).reset_index()\n",
    "fundcounter_df = fundcounter_df.sort_values(by=['companyid', 'mydate', 'id'])\n",
    "fundcounter_df['fund_counter'] = fundcounter_df.groupby('companyid').cumcount() + 1\n",
    "fundcounter_df = fundcounter_df[['companyid', 'id', 'fund_counter']]\n",
    "\n",
    "# Save fundcounter data\n",
    "fundcounter_df.to_csv('fundcounter.csv', index=False)\n",
    "\n",
    "# Step 4: Create a database of error terms from the 7-factor regressions\n",
    "potential_divcorr['epsilon'] = potential_divcorr['excess_ret'] - potential_divcorr['alpha']\n",
    "epsilon_df = potential_divcorr[['id', 'mydate', 'epsilon']]\n",
    "\n",
    "# Save epsilon data\n",
    "epsilon_df.to_csv('epsilon.csv', index=False)\n",
    "potential_divcorr.to_csv('potential_divcorr.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T12:18:02.489143Z",
     "start_time": "2024-06-28T12:18:02.476806Z"
    }
   },
   "id": "f752094e59adfb00",
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T12:18:02.662760Z",
     "start_time": "2024-06-28T12:18:02.658131Z"
    }
   },
   "id": "6dba2ba88b23744c",
   "execution_count": 66
  },
  {
   "cell_type": "markdown",
   "source": [
    "RET_CORR_BIG"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d181b256883a4d83"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "import glob\n",
    "import logging\n",
    "\n",
    "# Initialize the logging functionality\n",
    "logging.basicConfig(filename='ret_corr_big.log', level=logging.INFO)\n",
    "\n",
    "# Load datasets\n",
    "potential_divcorr = pd.read_csv('potential_divcorr.csv')\n",
    "epsilon = pd.read_csv('epsilon.csv')\n",
    "simulcounter = pd.read_csv('simulcounter.csv')\n",
    "fundcounter = pd.read_csv('fundcounter.csv')\n",
    "\n",
    "# Filter the data and merge necessary information\n",
    "potential_divcorr = potential_divcorr[['companyid', 'mydate', 'id', 'ret', 'excess_ret', 'alpha']]\n",
    "potential_divcorr = potential_divcorr.sort_values(by=['id', 'mydate'])\n",
    "potential_divcorr = potential_divcorr.merge(epsilon, on=['id', 'mydate'], how='left')\n",
    "potential_divcorr = potential_divcorr.merge(simulcounter, on='companyid', how='inner')\n",
    "\n",
    "# Filter firms with at least two funds simultaneously\n",
    "potential_divcorr = potential_divcorr[potential_divcorr['simulcounter'] > 1]\n",
    "\n",
    "# Pivot data to wide format using actual IDs\n",
    "df_wide = potential_divcorr.pivot_table(index=[\"companyid\", \"mydate\"], columns=\"id\", values=[\"ret\", \"epsilon\"], aggfunc='first')\n",
    "\n",
    "# Flatten multi-index columns\n",
    "df_wide.columns = [f\"{col[0]}_{col[1]}\" for col in df_wide.columns]\n",
    "df_wide.reset_index(inplace=True)\n",
    "\n",
    "# Initialize the parameter `last`\n",
    "last = 65\n",
    "\n",
    "# Function to calculate and save correlations using actual IDs\n",
    "def calculate_and_save_correlations(df_wide, last):\n",
    "    results = []\n",
    "    columns = df_wide.columns\n",
    "    fund_ids = [col.split(\"_\")[1] for col in columns if \"ret_\" in col]\n",
    "    unique_fund_ids = sorted(set(fund_ids))\n",
    "    \n",
    "    for idx1 in range(len(unique_fund_ids)):\n",
    "        fund_id1 = unique_fund_ids[idx1]\n",
    "        for idx2 in range(idx1 + 1, len(unique_fund_ids)):\n",
    "            if idx2 - idx1 >= last:\n",
    "                break\n",
    "            fund_id2 = unique_fund_ids[idx2]\n",
    "            relevant_columns = [f\"ret_{fund_id1}\", f\"ret_{fund_id2}\", f\"epsilon_{fund_id1}\", f\"epsilon_{fund_id2}\"]\n",
    "            if all(col in df_wide.columns for col in relevant_columns):\n",
    "                temp_df = df_wide[['companyid', 'mydate'] + relevant_columns].dropna()\n",
    "                if len(temp_df) >= 12:\n",
    "                    correlations = temp_df.corr()\n",
    "                    ret_corr = correlations.loc[f\"ret_{fund_id1}\", f\"ret_{fund_id2}\"]\n",
    "                    epsilon_corr = correlations.loc[f\"epsilon_{fund_id1}\", f\"epsilon_{fund_id2}\"]\n",
    "                    results.append({\n",
    "                        \"companyid\": temp_df['companyid'].iloc[0],\n",
    "                        \"div_cons1\": fund_id1,\n",
    "                        \"div_cons2\": fund_id2,\n",
    "                        \"div_corr\": ret_corr,\n",
    "                        \"corr_eps\": epsilon_corr\n",
    "                    })\n",
    "                    # Save results to CSV, one file per pair\n",
    "                    result_df = pd.DataFrame([results[-1]])\n",
    "                    result_df.to_csv(f\"div_corr_{fund_id1}_{fund_id2}.csv\", index=False)\n",
    "\n",
    "# Calculate and save correlations\n",
    "calculate_and_save_correlations(df_wide, last)\n",
    "\n",
    "# Combine all correlation files into a single DataFrame\n",
    "complete_divcorr = pd.DataFrame()\n",
    "correlation_files = glob.glob(\"div_corr_*.csv\")\n",
    "for file in correlation_files:\n",
    "    temp_df = pd.read_csv(file)\n",
    "    complete_divcorr = pd.concat([complete_divcorr, temp_df], ignore_index=True)\n",
    "    complete_divcorr.sort_values(by=['companyid', 'div_cons1', 'div_cons2'], inplace=True)\n",
    "    complete_divcorr['div_corr_id'] = range(1, len(complete_divcorr)+1)\n",
    "complete_divcorr.to_csv(\"complete_divcorr.csv\", index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T12:18:03.259754Z",
     "start_time": "2024-06-28T12:18:03.069196Z"
    }
   },
   "id": "5ea412d96e56ea00",
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[68], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m n \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m----> 4\u001B[0m     os\u001B[38;5;241m.\u001B[39mremove(\u001B[43mglob\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdiv_corr_*.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m:\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n",
      "\u001B[0;31mTypeError\u001B[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "for x in range(1, 100000):\n",
    "    n = x + 1\n",
    "    try:\n",
    "        os.remove(glob('div_corr_*.csv'))\n",
    "    except FileNotFoundError:\n",
    "        continue"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T12:18:03.278612Z",
     "start_time": "2024-06-28T12:18:03.263425Z"
    }
   },
   "id": "2391a814e511c740",
   "execution_count": 68
  },
  {
   "cell_type": "markdown",
   "source": [
    "RET_CORR_INTEGRATE -> zit al bij BIG "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbe2bcdbed64671c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Enig hier is dat ipv tm 65, de laatste 65 worden gepakt, nog niet helemaal duidelijk wat het uitmaakt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "231fb5954cc552fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "RET_CORR_INTEGRATE 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c53a99283643eb27"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jy/cfb14z9n637_y0c4jp4g2j1h0000gn/T/ipykernel_29273/3981866167.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  potential_treat2['closedxcrisis'].fillna(0, inplace=True)\n",
      "/var/folders/jy/cfb14z9n637_y0c4jp4g2j1h0000gn/T/ipykernel_29273/3981866167.py:46: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  batch3['corr_eps'].replace(-999, np.nan, inplace=True)\n",
      "/var/folders/jy/cfb14z9n637_y0c4jp4g2j1h0000gn/T/ipykernel_29273/3981866167.py:95: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  integrate2['corr_eps'].replace(-999, np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load necessary datasets\n",
    "complete_divcorr = pd.read_csv('complete_divcorr.csv')\n",
    "potential_treat0 = pd.read_csv('potential_treat0.csv')\n",
    "\n",
    "# Step 1: Make a list of every fund (id) in complete_divcorr and call it potential_treat1\n",
    "divcorr_id1 = complete_divcorr[['div_cons1']].rename(columns={'div_cons1': 'id'})\n",
    "divcorr_id2 = complete_divcorr[['div_cons2']].rename(columns={'div_cons2': 'id'})\n",
    "potential_treat1 = pd.concat([divcorr_id1, divcorr_id2]).drop_duplicates().sort_values(by='id')\n",
    "potential_treat1.to_csv('potential_treat1.csv', index=False)\n",
    "\n",
    "# Step 2: Merge in everything from potential_treat0\n",
    "potential_treat2 = potential_treat0.merge(potential_treat1, on='id', how='left')\n",
    "potential_treat2['closedxcrisis'].fillna(0, inplace=True)\n",
    "potential_treat2['closedxcrisis'] = potential_treat2['closedxcrisis'].astype(int)\n",
    "potential_treat2['fund_closedxcrisis'] = potential_treat2.groupby('id')['closedxcrisis'].transform('max')\n",
    "potential_treat2.to_csv('potential_treat2.csv', index=False)\n",
    "\n",
    "# Step 3: Create a file of ids with min_mydate\n",
    "minmydate = potential_treat2[potential_treat2['mydate'] == potential_treat2['min_mydate']]\n",
    "minmydate = minmydate[['id', 'min_mydate']]\n",
    "minmydate.to_csv('minmydate.csv', index=False)\n",
    "\n",
    "oth_minmydate = minmydate.rename(columns={'id': 'other_fund', 'min_mydate': 'oth_minmydate'})\n",
    "oth_minmydate.to_csv('oth_minmydate.csv', index=False)\n",
    "\n",
    "# Step 4: Eliminate \"twins\"\n",
    "# Batch 1\n",
    "batch1 = complete_divcorr[['companyid', 'div_cons1', 'div_cons2', 'div_corr', 'corr_eps']]\n",
    "batch1 = batch1.rename(columns={'div_cons1': 'id', 'div_cons2': 'other_fund'})\n",
    "batch1.to_csv('batch1.csv', index=False)\n",
    "\n",
    "# Batch 2\n",
    "batch2 = complete_divcorr[['companyid', 'div_cons1', 'div_cons2', 'div_corr', 'corr_eps']]\n",
    "batch2 = batch2.rename(columns={'div_cons2': 'id', 'div_cons1': 'other_fund'})\n",
    "batch2 = pd.concat([batch2, batch1]).drop_duplicates().sort_values(by=['companyid', 'id', 'other_fund'])\n",
    "\n",
    "batch2 = batch2.merge(minmydate, on='id', how='inner').rename(columns={'min_mydate': 'id_minmydate'})\n",
    "batch2 = batch2.merge(oth_minmydate, on='other_fund', how='inner')\n",
    "batch2.to_csv('batch2.csv', index=False)\n",
    "\n",
    "# Batch 3: Focus on (unique) funds with div_corr > 0.985\n",
    "batch3 = batch2.copy()\n",
    "batch3['corr_eps'].replace(-999, np.nan, inplace=True)\n",
    "batch3 = batch3[batch3['div_corr'] > 0.985]\n",
    "batch3.drop_duplicates(subset=['id', 'other_fund'], inplace=True)\n",
    "\n",
    "batch3['keep_id'] = np.where(batch3['id_minmydate'] <= batch3['oth_minmydate'], 1, 0)\n",
    "batch3['keep_other_fund'] = np.where(batch3['oth_minmydate'] < batch3['id_minmydate'], 1, 0)\n",
    "batch3.to_csv('batch3.csv', index=False)\n",
    "\n",
    "# Batch 4: Identify other_fund ids when keep_id==0 for an id and rename other_fund id\n",
    "batch4 = batch3[batch3['keep_id'] == 0][['other_fund', 'div_corr']]\n",
    "batch4 = batch4.rename(columns={'other_fund': 'id'})\n",
    "batch4.to_csv('batch4.csv', index=False)\n",
    "\n",
    "# Highcorr_set: Set of funds to be kept, even though they are highly correlated\n",
    "highcorr_set = pd.concat([\n",
    "    batch3[batch3['keep_other_fund'] == 0][['id', 'div_corr']],\n",
    "    batch4\n",
    "]).drop_duplicates(subset=['id']).sort_values(by='id')\n",
    "highcorr_set.to_csv('highcorr_set.csv', index=False)\n",
    "\n",
    "# Dropcorr_set: Set of \"twin\" funds to be dropped\n",
    "dropcorr_set = batch3[['id']].drop_duplicates().sort_values(by='id')\n",
    "dropcorr_set = dropcorr_set.merge(highcorr_set, on='id', how='left', indicator=True)\n",
    "dropcorr_set = dropcorr_set[dropcorr_set['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "dropcorr_set.to_csv('dropcorr_set.csv', index=False)\n",
    "\n",
    "# Define the test set\n",
    "intheset = potential_treat2.copy()\n",
    "intheset = intheset.merge(dropcorr_set, on='id', how='left', indicator=True)\n",
    "intheset['intheset'] = 1\n",
    "intheset.loc[intheset['crisis'] == 1, 'intheset'] = 0\n",
    "intheset.loc[intheset['fund_closedxcrisis'] == 1, 'intheset'] = 0\n",
    "intheset.loc[intheset['min_mydate'] > 562, 'intheset'] = 0\n",
    "intheset.loc[intheset['_merge'] == 'both', 'intheset'] = 0\n",
    "intheset.drop(columns=['_merge'], inplace=True)\n",
    "intheset = intheset[intheset['intheset'] == 1]\n",
    "\n",
    "intheset['intheset2'] = np.where((intheset['mydate'] <= 625) & (intheset['mydate'] >= 617), 0, intheset['intheset'])\n",
    "intheset['counter'] = intheset.groupby('id').cumcount() + 1\n",
    "intheset['maxcounter'] = intheset.groupby('id')['counter'].transform('max')\n",
    "intheset['intheset'] = np.where(intheset['maxcounter'] < 12, 0, intheset['intheset'])\n",
    "\n",
    "intheset = intheset[intheset['intheset'] == 1][['id', 'mydate', 'intheset', 'intheset2']]\n",
    "intheset.to_csv('intheset.csv', index=False)\n",
    "\n",
    "# Bring in all pairwise correlations\n",
    "integrate2 = complete_divcorr[['companyid', 'div_cons1', 'div_cons2', 'div_corr', 'corr_eps']]\n",
    "integrate2 = integrate2.rename(columns={'div_cons2': 'id', 'div_cons1': 'other_fund'})\n",
    "integrate2 = pd.concat([integrate2, batch1]).drop_duplicates().sort_values(by=['companyid', 'id', 'other_fund'])\n",
    "integrate2['corr_eps'].replace(-999, np.nan, inplace=True)\n",
    "integrate2.to_csv('integrate2.csv', index=False)\n",
    "\n",
    "# Identify funds with div_corr that were closed during the crisis\n",
    "closed = potential_treat2[potential_treat2['closedxcrisis'] == 1][['id']]\n",
    "closed.to_csv('closed.csv', index=False)\n",
    "\n",
    "# Merge closed fund with list of funds w/div_corr to find the \"real\" treatments\n",
    "integrate3 = closed.merge(integrate2, on='id', how='inner')\n",
    "integrate3['treatment99'] = np.where(integrate3['div_corr'] < 0.985, 1, 0)\n",
    "integrate3['treatment90'] = np.where(integrate3['div_corr'] < 0.895, 1, 0)\n",
    "integrate3['treatment99_eps'] = np.where((integrate3['corr_eps'] < 0.985) & (integrate3['corr_eps'].notna()), 1, 0)\n",
    "integrate3['treatment90_eps'] = np.where((integrate3['corr_eps'] < 0.895) & (integrate3['corr_eps'].notna()), 1, 0)\n",
    "\n",
    "integrate3 = integrate3.rename(columns={'id': 'treated_id', 'other_fund': 'id'}).sort_values(by=['companyid', 'id'])\n",
    "integrate3.to_csv('integrate3.csv', index=False)\n",
    "\n",
    "# Merge with potential_treat2\n",
    "integrate3 = integrate3.merge(potential_treat2, on=['companyid', 'id'], how='outer')\n",
    "\n",
    "# DIV_CORR < 0.985\n",
    "integrate3['closedxcrisis2'] = integrate3['closedxcrisis']\n",
    "integrate3['closedxcrisis3'] = integrate3['closedxcrisis']\n",
    "integrate3['closedxcrisis4'] = integrate3['closedxcrisis']\n",
    "integrate3.loc[integrate3['div_corr'] >= 0.985, 'closedxcrisis'] = 0\n",
    "integrate3['fund_closedxcrisis'] = integrate3.groupby('id')['closedxcrisis'].transform('max')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T12:18:05.318188Z",
     "start_time": "2024-06-28T12:18:05.234454Z"
    }
   },
   "id": "3c9f278779252fd1",
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integration completed and temporary files cleaned up.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Instantaneous and constant FIRM measures of closure during the crisis\n",
    "integrate3['firm_closedxcrisis'] = integrate3.groupby(['companyid', 'mydate'])['closedxcrisis'].transform('max')\n",
    "integrate3['max_firm_closedxcrisis'] = integrate3.groupby('companyid')['closedxcrisis'].transform('max')\n",
    "\n",
    "# A fund cannot be treated if it started within one year of the financial crisis\n",
    "integrate3.loc[integrate3['min_mydate'] > 562, 'max_firm_closedxcrisis'] = 0\n",
    "integrate3.loc[integrate3['div_corr'] >= 0.985, 'max_firm_closedxcrisis'] = 0\n",
    "\n",
    "# crisis_treat24 equals one for two years after the end of the crisis\n",
    "integrate3['crisis_treat24'] = 0\n",
    "integrate3.loc[(integrate3['max_firm_closedxcrisis'] == 1) & (integrate3['mydate'] >= 595) & (integrate3['mydate'] <= 619), 'crisis_treat24'] = 1\n",
    "\n",
    "# DIV_CORR < 0.895\n",
    "integrate3.loc[integrate3['div_corr'] >= 0.895, 'closedxcrisis2'] = 0\n",
    "integrate3['fund_closedxcrisis2'] = integrate3.groupby('id')['closedxcrisis2'].transform('max')\n",
    "\n",
    "# Instantaneous and constant FIRM measures of closure during the crisis\n",
    "integrate3['firm_closedxcrisis2'] = integrate3.groupby(['companyid', 'mydate'])['closedxcrisis2'].transform('max')\n",
    "integrate3['max_firm_closedxcrisis2'] = integrate3.groupby('companyid')['closedxcrisis2'].transform('max')\n",
    "\n",
    "# A fund cannot be treated if it started within one year of the financial crisis\n",
    "integrate3.loc[integrate3['min_mydate'] > 562, 'max_firm_closedxcrisis2'] = 0\n",
    "integrate3.loc[integrate3['div_corr'] >= 0.895, 'max_firm_closedxcrisis2'] = 0\n",
    "\n",
    "# crisis_treat24 equals one for two years after the end of the crisis\n",
    "integrate3['crisis_treat24_90'] = 0\n",
    "integrate3.loc[(integrate3['max_firm_closedxcrisis2'] == 1) & (integrate3['mydate'] >= 595) & (integrate3['mydate'] <= 619), 'crisis_treat24_90'] = 1\n",
    "\n",
    "# CORR_EPS < 0.985\n",
    "integrate3.loc[integrate3['corr_eps'] >= 0.985, 'closedxcrisis3'] = 0\n",
    "integrate3['fund_closedxcrisis3'] = integrate3.groupby('id')['closedxcrisis3'].transform('max')\n",
    "\n",
    "# Instantaneous and constant FIRM measures of closure during the crisis\n",
    "integrate3['firm_closedxcrisis3'] = integrate3.groupby(['companyid', 'mydate'])['closedxcrisis3'].transform('max')\n",
    "integrate3['max_firm_closedxcrisis3'] = integrate3.groupby('companyid')['closedxcrisis3'].transform('max')\n",
    "\n",
    "# A fund cannot be treated if it started within one year of the financial crisis\n",
    "integrate3.loc[integrate3['min_mydate'] > 562, 'max_firm_closedxcrisis3'] = 0\n",
    "integrate3.loc[integrate3['div_corr'] >= 0.985, 'max_firm_closedxcrisis3'] = 0\n",
    "\n",
    "# crisis_treat24 equals one for two years after the end of the crisis\n",
    "integrate3['crisis_treat24_99eps'] = 0\n",
    "integrate3.loc[(integrate3['max_firm_closedxcrisis3'] == 1) & (integrate3['mydate'] >= 595) & (integrate3['mydate'] <= 619), 'crisis_treat24_99eps'] = 1\n",
    "\n",
    "# CORR_EPS < 0.895\n",
    "integrate3.loc[integrate3['corr_eps'] >= 0.895, 'closedxcrisis4'] = 0\n",
    "integrate3['fund_closedxcrisis4'] = integrate3.groupby('id')['closedxcrisis4'].transform('max')\n",
    "\n",
    "# Instantaneous and constant FIRM measures of closure during the crisis\n",
    "integrate3['firm_closedxcrisis4'] = integrate3.groupby(['companyid', 'mydate'])['closedxcrisis4'].transform('max')\n",
    "integrate3['max_firm_closedxcrisis4'] = integrate3.groupby('companyid')['closedxcrisis4'].transform('max')\n",
    "\n",
    "# A fund cannot be treated if it started within one year of the financial crisis\n",
    "integrate3.loc[integrate3['min_mydate'] > 562, 'max_firm_closedxcrisis4'] = 0\n",
    "integrate3.loc[integrate3['div_corr'] >= 0.895, 'max_firm_closedxcrisis4'] = 0\n",
    "\n",
    "# crisis_treat24 equals one for two years after the end of the crisis\n",
    "integrate3['crisis_treat24_90eps'] = 0\n",
    "integrate3.loc[(integrate3['max_firm_closedxcrisis4'] == 1) & (integrate3['mydate'] >= 595) & (integrate3['mydate'] <= 619), 'crisis_treat24_90eps'] = 1\n",
    "\n",
    "integrate3.to_csv('integrate4.csv', index=False)\n",
    "\n",
    "# Clean up\n",
    "import os\n",
    "files_to_erase = [\n",
    "    'batch1.csv', 'batch3.csv', 'batch4.csv', 'potential_treat1.csv', \n",
    "    'integrate2.csv', 'highcorr_set.csv'\n",
    "]\n",
    "\n",
    "for file in files_to_erase:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "\n",
    "print(\"Integration completed and temporary files cleaned up.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T12:18:05.432542Z",
     "start_time": "2024-06-28T12:18:05.397557Z"
    }
   },
   "id": "e5771ae3a8376d14",
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T12:18:05.603916Z",
     "start_time": "2024-06-28T12:18:05.591086Z"
    }
   },
   "id": "515bac032aacf35",
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-28T12:18:05.771607Z",
     "start_time": "2024-06-28T12:18:05.748551Z"
    }
   },
   "id": "e591e0e8a74924f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "311451896696bfa4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
