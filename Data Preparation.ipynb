{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "MAKEHF1DATA"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ac9823ff687378d"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-18T02:57:01.578087500Z",
     "start_time": "2024-06-18T02:56:21.792234700Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load the merged data\n",
    "merged_df = pd.read_csv('merged_df.csv', low_memory=False)\n",
    "\n",
    "# Load the mydate converter data\n",
    "mydate_converter = pd.read_csv('mydate_converter.csv', names=['year', 'month', 'mydate'], skiprows=[0])\n",
    "\n",
    "# Remove non-numeric rows from mydate_converter\n",
    "mydate_converter = mydate_converter[pd.to_numeric(mydate_converter['year'], errors='coerce').notnull()]\n",
    "mydate_converter['year'] = mydate_converter['year'].astype(int)\n",
    "mydate_converter['month'] = mydate_converter['month'].astype(int)\n",
    "mydate_converter['mydate'] = mydate_converter['mydate'].astype(int)\n",
    "\n",
    "# Extract year and month from the date in merged_df\n",
    "merged_df['year'] = pd.to_numeric(merged_df['date'].str[:4], errors='coerce')\n",
    "merged_df['month'] = pd.to_numeric(merged_df['date'].str[5:7], errors='coerce')\n",
    "\n",
    "# Ensure the year and month columns are of the same data type\n",
    "merged_df['year'] = merged_df['year'].astype(int)\n",
    "merged_df['month'] = merged_df['month'].astype(int)\n",
    "\n",
    "# Merge with mydate converter\n",
    "merged_df = merged_df.merge(mydate_converter, on=['year', 'month'], how='left')\n",
    "\n",
    "# Step 2: Filter and Clean the Data\n",
    "# Keep records from 1994 onwards\n",
    "merged_df = merged_df[merged_df['year'] >= 1994]\n",
    "\n",
    "# Generate elapsed time\n",
    "merged_df['maxmydate'] = pd.to_numeric(merged_df.groupby('id')['mydate'].transform('max'), errors='coerce')\n",
    "merged_df['minmydate'] = pd.to_numeric(merged_df.groupby('id')['mydate'].transform('min'), errors='coerce')\n",
    "merged_df['elapsedtime'] = merged_df['maxmydate'] - merged_df['minmydate'] + 1\n",
    "\n",
    "# Filter out invalid records\n",
    "merged_df = merged_df[(merged_df['aum'] <= 100000000000) & (merged_df['aum'] >= 1000000)]\n",
    "merged_df = merged_df[merged_df['ret'] <= 1000]\n",
    "\n",
    "# Identify sporadic reporters\n",
    "retcounter = merged_df.groupby('id').size().reset_index(name='ret_counter')\n",
    "\n",
    "merged_df = merged_df.merge(retcounter, on='id', how='left')\n",
    "merged_df['sporadic_dum'] = (merged_df['elapsedtime'] > merged_df['ret_counter']).astype(int)\n",
    "max_sporadic = merged_df.groupby('id')['sporadic_dum'].transform('max')\n",
    "merged_df = merged_df[max_sporadic == 0]\n",
    "\n",
    "# Drop funds with fewer than 12 months of data\n",
    "merged_df = merged_df[merged_df['ret_counter'] >= 12]\n",
    "merged_df = merged_df[merged_df['ret'].notna()]\n",
    "\n",
    "# Drop unnecessary columns\n",
    "merged_df.drop(columns=['elapsedtime', 'ret_counter'], inplace=True)\n",
    "\n",
    "# Save the cleaned data\n",
    "merged_df.to_csv('tass2.csv', index=False)\n",
    "\n",
    "# Step 3: Break Data into Pre-Crisis, Crisis, and Post-Crisis Periods\n",
    "# Split data into pre-crisis, crisis, and post-crisis periods\n",
    "tass_pre = merged_df[merged_df['mydate'] < 574]\n",
    "tass_pre.to_csv('tass2_pre.csv', index=False)\n",
    "\n",
    "tass_crisis = merged_df[(merged_df['mydate'] >= 574) & (merged_df['mydate'] <= 593)]\n",
    "tass_crisis.to_csv('tass2_crisis.csv', index=False)\n",
    "\n",
    "tass_post = merged_df[merged_df['mydate'] > 593]\n",
    "tass_post.to_csv('tass2_post.csv', index=False)\n",
    "\n",
    "# Step 4: Perform AR1 Adjustment\n",
    "def ar1_adjustment(df, period_name):\n",
    "    df = df.sort_values(by=['id', 'mydate'])\n",
    "    df['rho'] = 0.0\n",
    "    unique_ids = df['id'].unique()\n",
    "\n",
    "    for unique_id in unique_ids:\n",
    "        sub_df = df[df['id'] == unique_id]\n",
    "        if len(sub_df) > 1:\n",
    "            model = sm.OLS(sub_df['ret'].iloc[1:], sm.add_constant(sub_df['ret'].shift(1).iloc[1:])).fit()\n",
    "            rho = model.params.iloc[1] if len(model.params) > 1 else 0  # Use iloc for positional access\n",
    "            df.loc[df['id'] == unique_id, 'rho'] = rho\n",
    "\n",
    "    df['ret_star'] = (df['ret'] - df['rho'] * df['ret'].shift(1)) / (1 - df['rho'])\n",
    "    df.to_csv(f'tass4_{period_name}.csv', index=False)\n",
    "    return df\n",
    "\n",
    "# AR1 adjustment for pre-crisis period\n",
    "tass_pre_adjusted = ar1_adjustment(tass_pre, 'pre')\n",
    "\n",
    "# AR1 adjustment for crisis period\n",
    "tass_crisis_adjusted = ar1_adjustment(tass_crisis, 'crisis')\n",
    "\n",
    "# AR1 adjustment for post-crisis period\n",
    "tass_post_adjusted = ar1_adjustment(tass_post, 'post')\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year  month     DGS10  mydate\n",
      "0  1993      1  6.600000     396\n",
      "1  1993      2  6.258947     397\n",
      "2  1993      3  5.975217     398\n",
      "3  1993      4  5.969524     399\n",
      "4  1993      5  6.035500     400\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def load_and_clean_data_with_mydate(file_path, mydate_converter, keep_columns=None, drop_na_columns=None):\n",
    "    file_ext = file_path.split('.')[-1]\n",
    "    if file_ext == 'csv':\n",
    "        df = pd.read_csv(file_path)\n",
    "    elif file_ext in ['xls', 'xlsx']:\n",
    "        df = pd.read_excel(file_path)\n",
    "\n",
    "    # Merge with mydate_converter\n",
    "    mydate_converter = mydate_converter[pd.to_numeric(mydate_converter['year'], errors='coerce').notnull()]\n",
    "\n",
    "    mydate_converter['year'] = mydate_converter['year'].astype(int)\n",
    "    mydate_converter['month'] = mydate_converter['month'].astype(int)\n",
    "    mydate_converter['mydate'] = mydate_converter['mydate'].astype(int)\n",
    "\n",
    "    df = df.merge(mydate_converter, on=['year', 'month'], how='left')\n",
    "    \n",
    "    if drop_na_columns:\n",
    "        df = df.dropna(subset=drop_na_columns)\n",
    "    if keep_columns:\n",
    "        df = df[keep_columns]\n",
    "    \n",
    "    df = df.sort_values(by=['year', 'month'])\n",
    "    return df\n",
    "\n",
    "# Load the mydate converter data\n",
    "mydate_converter = pd.read_csv('mydate_converter.csv', names=['year', 'month', 'mydate'], skiprows=[0])\n",
    "\n",
    "# Load factor files with mydate\n",
    "df_ff = load_and_clean_data_with_mydate('Factors/Corrected_FF_Research_Data_Factors.csv', mydate_converter, drop_na_columns=['month'])\n",
    "df_fung_hsieh = load_and_clean_data_with_mydate('Factors/TF-Fac.xlsx', mydate_converter, keep_columns=['PTFSBD', 'PTFSFX', 'PTFSCOM', 'year', 'month'], drop_na_columns=['year'])\n",
    "df_mom = load_and_clean_data_with_mydate('Factors/Corrected_FF_Momentum_Factor.csv', mydate_converter)\n",
    "df_bond = load_and_clean_data_with_mydate('Factors/DBAA_Monthly_Averages.csv', mydate_converter)\n",
    "df_credit = load_and_clean_data_with_mydate('Factors/DGS10_Monthly_Averages.csv', mydate_converter, drop_na_columns=['year'])\n",
    "print(df_credit.head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T02:57:01.721108300Z",
     "start_time": "2024-06-18T02:57:01.588942400Z"
    }
   },
   "id": "8292d8526a46cf22",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to perform AR1 adjustment\n",
    "def ar1_adjustment(df, period_name):\n",
    "    df = df.copy()\n",
    "    df['ret_star'] = df['ret']\n",
    "    unique_ids = df['id'].unique()\n",
    "    for unique_id in unique_ids:\n",
    "        subset = df[df['id'] == unique_id]\n",
    "        if len(subset) > 1:\n",
    "            subset = subset.sort_values(by='mydate')\n",
    "            X = sm.add_constant(subset['ret'].shift(1).dropna())\n",
    "            y = subset['ret'].iloc[1:]\n",
    "            if len(X) == len(y):  # Ensure X and y have the same length\n",
    "                try:\n",
    "                    model = sm.OLS(y, X).fit()\n",
    "                    rho = model.params.iloc[1] if len(model.params) > 1 else 0  # Default to 0 if model fitting fails\n",
    "                    df.loc[df['id'] == unique_id, 'ret_star'] = (df['ret'] - rho * df['ret'].shift(1)) / (1 - rho)\n",
    "                except Exception as e:\n",
    "                    print(f\"Model fitting failed for id {unique_id} with error: {e}\")\n",
    "    df.to_csv(f'tass4_{period_name}.csv', index=False)\n",
    "    return df\n",
    "\n",
    "# Load merged data\n",
    "\n",
    "merged_df['mydate'] = merged_df['mydate'].astype(int)\n",
    "\n",
    "# Define periods\n",
    "pre_crisis_period = merged_df[(merged_df['mydate'] >= 0) & (merged_df['mydate'] < 575)]\n",
    "crisis_period = merged_df[(merged_df['mydate'] >= 575) & (merged_df['mydate'] <= 593)]\n",
    "post_crisis_period = merged_df[(merged_df['mydate'] > 593)]\n",
    "\n",
    "# Perform AR1 adjustment\n",
    "tass4_pre = ar1_adjustment(pre_crisis_period, 'pre')\n",
    "tass4_crisis = ar1_adjustment(crisis_period, 'crisis')\n",
    "tass4_post = ar1_adjustment(post_crisis_period, 'post')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T02:57:58.905803700Z",
     "start_time": "2024-06-18T02:57:01.726658400Z"
    }
   },
   "id": "7c101f2e181229ba",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to merge factors with TASS data\n",
    "def merge_factors(df_tass, factors_list):\n",
    "    for factor_df in factors_list:\n",
    "        df_tass = pd.merge(df_tass, factor_df, on=['year', 'month'], how='left', indicator=True, suffixes=('','_remove'))\n",
    "        df_tass.drop([i for i in df_tass.columns if 'remove' in i], axis=1, inplace=True)\n",
    "        df_tass = df_tass[df_tass['_merge'] == 'both'].drop('_merge', axis=1)\n",
    "    return df_tass\n",
    "\n",
    "# Load the adjusted TASS data for each period\n",
    "df_tass4_pre = pd.read_csv('tass4_pre.csv')\n",
    "df_tass4_crisis = pd.read_csv('tass4_crisis.csv')\n",
    "df_tass4_post = pd.read_csv('tass4_post.csv')\n",
    "\n",
    "# List of factor dataframes\n",
    "factors_list = [df_ff, df_fung_hsieh, df_mom, df_bond, df_credit]\n",
    "\n",
    "# Merging factors with TASS data for each period\n",
    "df_tass4_pre = merge_factors(df_tass4_pre, factors_list)\n",
    "df_tass4_crisis = merge_factors(df_tass4_crisis, factors_list)\n",
    "df_tass4_post = merge_factors(df_tass4_post, factors_list)\n",
    "\n",
    "# Save the merged dataframes to CSV for further use\n",
    "df_tass4_pre.to_csv('tass4_pre_merged.csv', index=False)\n",
    "df_tass4_crisis.to_csv('tass4_crisis_merged.csv', index=False)\n",
    "df_tass4_post.to_csv('tass4_post_merged.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T03:01:07.394499100Z",
     "start_time": "2024-06-18T03:01:01.285211900Z"
    }
   },
   "id": "253f360806dc1828",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_tass4_pre' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 41\u001B[0m\n\u001B[0;32m     38\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n\u001B[0;32m     40\u001B[0m \u001B[38;5;66;03m# Perform asset pricing analysis for each period\u001B[39;00m\n\u001B[1;32m---> 41\u001B[0m tass5_pre \u001B[38;5;241m=\u001B[39m asset_pricing(df_tass4_pre, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpre\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     42\u001B[0m tass5_crisis \u001B[38;5;241m=\u001B[39m asset_pricing(df_tass4_crisis, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcrisis\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     43\u001B[0m tass5_post \u001B[38;5;241m=\u001B[39m asset_pricing(df_tass4_post, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'df_tass4_pre' is not defined"
     ]
    }
   ],
   "source": [
    "def asset_pricing(df, period_name):\n",
    "    df['lhs'] = df['ret_star'] - df['RF']\n",
    "    df['excess_ret'] = 0\n",
    "    df['beta1'] = 0.0\n",
    "    df['beta2'] = 0.0\n",
    "    df['beta3'] = 0.0\n",
    "    df['beta4'] = 0.0\n",
    "    df['beta5'] = 0.0\n",
    "    df['beta6'] = 0.0\n",
    "    df['beta7'] = 0.0\n",
    "    df['alpha'] = 0.0\n",
    "    df['stdv'] = 0.0\n",
    "    df['r2'] = 0.0\n",
    "\n",
    "    df.rename(columns={'mktrf': 'eq_prem'}, inplace=True)\n",
    "    unique_ids = df['id'].unique()\n",
    "\n",
    "    for unique_id in unique_ids:\n",
    "        sub_df = df[df['id'] == unique_id]\n",
    "        if len(sub_df) > 1:\n",
    "            model = sm.OLS(sub_df['lhs'], sm.add_constant(sub_df[['Mkt-RF', 'SMB', 'PTFSBD', 'PTFSFX', 'PTFSCOM', 'year', 'DBAA']])).fit()\n",
    "            df.loc[df['id'] == unique_id, 'r2'] = model.rsquared\n",
    "            predictions = model.predict(sm.add_constant(sub_df[['Mkt-RF', 'SMB', 'PTFSBD', 'PTFSFX', 'PTFSCOM', 'year', 'DBAA']]))\n",
    "            if len(model.params) > 1: df.loc[df['id'] == unique_id, 'beta1'] = model.params.iloc[1]\n",
    "            if len(model.params) > 2: df.loc[df['id'] == unique_id, 'beta2'] = model.params.iloc[2]\n",
    "            if len(model.params) > 3: df.loc[df['id'] == unique_id, 'beta3'] = model.params.iloc[3]\n",
    "            if len(model.params) > 4: df.loc[df['id'] == unique_id, 'beta4'] = model.params.iloc[4]\n",
    "            if len(model.params) > 5: df.loc[df['id'] == unique_id, 'beta5'] = model.params.iloc[5]\n",
    "            if len(model.params) > 6: df.loc[df['id'] == unique_id, 'beta6'] = model.params.iloc[6]\n",
    "            if len(model.params) > 7: df.loc[df['id'] == unique_id, 'beta7'] = model.params.iloc[7]\n",
    "            if len(model.params) > 0: df.loc[df['id'] == unique_id, 'alpha'] = model.params.iloc[0]\n",
    "            df.loc[df['id'] == unique_id, 'excess_ret'] = df['lhs'] - predictions + df['alpha']\n",
    "            df.loc[df['id'] == unique_id, 'stdv'] = df['excess_ret'].std()\n",
    "\n",
    "    df['excess_ret'] = df['excess_ret'].where(df['ret'].notna())\n",
    "    df['excess_ret'] = df['excess_ret'].where(df['alpha'].notna())\n",
    "    df.to_csv(f'tass5_{period_name}.csv', index=False)\n",
    "    return df\n",
    "\n",
    "# Perform asset pricing analysis for each period\n",
    "tass5_pre = asset_pricing(df_tass4_pre, 'pre')\n",
    "tass5_crisis = asset_pricing(df_tass4_crisis, 'crisis')\n",
    "tass5_post = asset_pricing(df_tass4_post, 'post')\n",
    "\n",
    "# Combine all periods into a single dataset\n",
    "tass5 = pd.concat([tass5_pre, tass5_crisis, tass5_post])\n",
    "tass5.to_csv('tass5.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T11:30:14.431571100Z",
     "start_time": "2024-06-18T11:30:14.355937200Z"
    }
   },
   "id": "2a4279672ff5af0c",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tass5_pre' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[30], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Preliminarily define \"closed\" funds during the crisis\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m tass5 \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([tass5_pre, tass5_crisis, tass5_post])\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Define the crisis and post periods\u001B[39;00m\n\u001B[0;32m      5\u001B[0m tass5[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcrisis\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'tass5_pre' is not defined"
     ]
    }
   ],
   "source": [
    "# Preliminarily define \"closed\" funds during the crisis\n",
    "tass5 = pd.concat([tass5_pre, tass5_crisis, tass5_post])\n",
    "\n",
    "# Define the crisis and post periods\n",
    "tass5['crisis'] = 0\n",
    "tass5.loc[(tass5['mydate'] >= 573) & (tass5['mydate'] <= 594), 'crisis'] = 1\n",
    "\n",
    "tass5['post'] = 0\n",
    "tass5.loc[tass5['mydate'] > 594, 'post'] = 1\n",
    "\n",
    "# Identify the max and min mydate for each fund\n",
    "tass5['max_mydate'] = tass5.groupby('id')['mydate'].transform('max')\n",
    "tass5['min_mydate'] = tass5.groupby('id')['mydate'].transform('min')\n",
    "\n",
    "# Define funds that closed during the crisis\n",
    "tass5['closedxcrisis'] = 0\n",
    "tass5.loc[(tass5['max_mydate'] >= 573) & (tass5['max_mydate'] <= 594) & (tass5['mydate'] == tass5['max_mydate']), 'closedxcrisis'] = 1\n",
    "\n",
    "# Define firms that closed at least one fund during the crisis\n",
    "tass5['firm_closedxcrisis'] = tass5.groupby('companyid')['closedxcrisis'].transform('max')\n",
    "\n",
    "# Save the potential treatment dataset\n",
    "tass5.to_csv('potential_treat0.csv', index=False)\n",
    "\n",
    "# Create a dataset of firms that closed at least one fund during the crisis\n",
    "potential_divcorr = tass5[tass5['firm_closedxcrisis'] == 1].drop(columns=['firm_closedxcrisis', 'closedxcrisis'])\n",
    "potential_divcorr.to_csv('potential_divcorr.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T11:30:07.269269300Z",
     "start_time": "2024-06-18T11:30:07.225178600Z"
    }
   },
   "id": "2b81de57e704d968",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Pre-define treatment\n",
    "diag1 = tass5.groupby('companyid').agg({'firm_closedxcrisis': 'max', 'mydate': 'max'}).reset_index()\n",
    "diag1.rename(columns={'firm_closedxcrisis': 'treated'}, inplace=True)\n",
    "diag1['firm_closed'] = 0\n",
    "diag1.loc[(diag1['mydate'] >= 573) & (diag1['mydate'] <= 593), 'firm_closed'] = 1\n",
    "diag1.to_csv('diag1.csv', index=False)\n",
    "\n",
    "diag2 = tass5.merge(diag1, on='companyid', how='left')\n",
    "diag2['mydate'] = diag2['mydate_x']\n",
    "diag2['mydate2'] = diag2['mydate']\n",
    "diag2 = diag2.groupby('id').agg({'firm_closed': 'max', 'closedxcrisis': 'max', 'treated': 'max', 'mydate': 'max', 'mydate2': 'min'}).reset_index()\n",
    "diag2['pre_treat'] = 0\n",
    "diag2.loc[(diag2['firm_closed'] == 0) & (diag2['treated'] == 1) & (diag2['closedxcrisis'] == 0) & (diag2['mydate2'] >= 562) & (diag2['mydate'] >= 605), 'pre_treat'] = 1\n",
    "diag2 = diag2[['id', 'pre_treat']]\n",
    "diag2.to_csv('diag2.csv', index=False)\n",
    "\n",
    "potential_treat0 = pd.read_csv('potential_treat0.csv')\n",
    "diag2 = pd.read_csv('diag2.csv')\n",
    "\n",
    "potential_treat0 = potential_treat0.merge(diag2, on='id', how='left').fillna(0)\n",
    "potential_treat0['pre_treat'] = potential_treat0['pre_treat'].astype(int)\n",
    "diag3 = potential_treat0.groupby('companyid').agg({'pre_treat': 'max'}).reset_index()\n",
    "diag3.to_csv('diag3.csv', index=False)\n",
    "\n",
    "# Clean up intermediate files\n",
    "import os\n",
    "\n",
    "files_to_delete = [\n",
    "    'dataff_fffactors.csv', 'dataff_fung_hsieh.csv', 'dataff_mom.csv', \n",
    "    'dataff_bond.csv', 'dataff_credit.csv', 'tass4_pre.csv', \n",
    "    'tass4_crisis.csv', 'tass4_post.csv', 'tass5_pre.csv', \n",
    "    'tass5_crisis.csv', 'tass5_post.csv'\n",
    "]\n",
    "\n",
    "for file in files_to_delete:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T03:11:10.018342400Z",
     "start_time": "2024-06-18T03:11:07.995273100Z"
    }
   },
   "id": "c383f3d88e8c307d",
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "source": [
    "CAR36"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ddcb8b07e6a1991"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('tass5.csv')\n",
    "\n",
    "# Ensure the dataframe is sorted by 'id' and 'mydate'\n",
    "df = df.sort_values(by=['id', 'mydate'])\n",
    "\n",
    "# Calculate the moving average of excess returns over the past 12 months\n",
    "df['moveave_ex'] = df.groupby('id')['excess_ret'].transform(lambda x: x.rolling(window=12, min_periods=1).mean())\n",
    "\n",
    "# Calculate the sum of squared deviations from the moving average\n",
    "def sum_sq_ex(group):\n",
    "    return ((group['excess_ret'] - group['moveave_ex'])**2).rolling(window=12, min_periods=1).sum()\n",
    "\n",
    "df['sum_sq_ex'] = df.groupby('id').apply(sum_sq_ex).reset_index(level=0, drop=True)\n",
    "\n",
    "# Calculate the variance of the moving average and the standard deviation\n",
    "df['var_ex_move'] = df['sum_sq_ex'] / 12\n",
    "df['stdv_ex_move'] = np.sqrt(df['var_ex_move'])\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['sum_sq_ex', 'var_ex_move'])\n",
    "\n",
    "# Generate a counter for each id\n",
    "df['counter'] = df.groupby('id').cumcount() + 1\n",
    "\n",
    "# Generate the retdum column\n",
    "df['retdum'] = np.where(df['excess_ret'].notna(), 1, 0)\n",
    "\n",
    "# Calculate retcounter for various lengths up to 36\n",
    "for i in range(1, 37):\n",
    "    df[f'retcounter{i}'] = df.groupby('id')['retdum'].transform(lambda x: x.shift(i).rolling(window=i, min_periods=1).sum())\n",
    "\n",
    "# Determine the maximum retcounter value for each row\n",
    "df['retcounter'] = df[[f'retcounter{i}' for i in range(1, 37)]].bfill(axis=1).iloc[:, 0]\n",
    "\n",
    "# Calculate the CAR values for various lengths up to 36\n",
    "for i in range(1, 37):\n",
    "    df[f'CAR{i}'] = df.groupby('id')['excess_ret'].transform(lambda x: x.shift(i).rolling(window=i, min_periods=1).sum())\n",
    "\n",
    "# Determine the appropriate CAR value based on retcounter\n",
    "df['CARstar'] = np.nan\n",
    "for i in range(1, 37):\n",
    "    df['CARstar'] = np.where(df['retcounter'] >= i, df[f'CAR{i}'], df['CARstar'])\n",
    "\n",
    "# Adjust retcounter values greater than 36 to 36\n",
    "df['retcounter'] = np.where(df['retcounter'] > 36, 36, df['retcounter'])\n",
    "\n",
    "# Calculate the average CAR over the period\n",
    "df['avgCAR36'] = df['CARstar'] / df['retcounter']\n",
    "\n",
    "# Save the intermediate result to a CSV file\n",
    "df.to_csv('car36a.csv', index=False)\n",
    "\n",
    "# Filter out rows where excess is missing\n",
    "df = df[df['excess_ret'].notna()]\n",
    "\n",
    "# Keep only necessary columns\n",
    "df = df[['id', 'mydate', 'avgCAR36', 'stdv_ex_move']]\n",
    "\n",
    "# Save the final result to a CSV file\n",
    "df.to_csv('car36.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T03:17:58.619086Z",
     "start_time": "2024-06-18T03:15:50.545027700Z"
    }
   },
   "id": "b48281ac24d9b559",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "79f8dc71f5d8a001"
  },
  {
   "cell_type": "markdown",
   "source": [
    "ALL_CLOSURES"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b8c779fd3864bc7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the mydate converter\n",
    "mydate_converter = pd.read_csv('mydate_converter.csv', header=None, names=['year', 'month', 'mydate'], skiprows=[0])\n",
    "\n",
    "# Load the tass5 dataset\n",
    "tass5 = pd.read_csv('tass5.csv')\n",
    "\n",
    "# Calculate min and max mydate for each fund\n",
    "tass5['min_mydate'] = tass5.groupby('id')['mydate'].transform('min')\n",
    "tass5['max_mydate'] = tass5.groupby('id')['mydate'].transform('max')\n",
    "\n",
    "# Generate the starts data\n",
    "starts = tass5[['id', 'min_mydate']].drop_duplicates()\n",
    "starts = starts.groupby('min_mydate').size().reset_index(name='starts')\n",
    "starts.rename(columns={'min_mydate': 'mydate'}, inplace=True)\n",
    "\n",
    "# Generate the closures data\n",
    "closures = tass5[['id', 'max_mydate']].drop_duplicates()\n",
    "closures = closures.groupby('max_mydate').size().reset_index(name='closures')\n",
    "closures.rename(columns={'max_mydate': 'mydate'}, inplace=True)\n",
    "\n",
    "# Merge starts and closures data\n",
    "open_close = pd.merge(starts, closures, on='mydate', how='outer').fillna(0)\n",
    "\n",
    "# Merge with mydate converter\n",
    "open_close = pd.merge(open_close, mydate_converter, on='mydate', how='left')\n",
    "\n",
    "# Save the open_close data to Excel\n",
    "open_close.to_excel('open_close_TASS2015.xlsx', index=False)\n",
    "\n",
    "# Aggregate data by year\n",
    "open_close_year = open_close.groupby('year').sum().reset_index()\n",
    "open_close_year = open_close_year[['year', 'starts', 'closures']]\n",
    "\n",
    "# Save the open_close_year data to Excel\n",
    "open_close_year.to_excel('open_close_TASS2015_year.xlsx', index=False)\n",
    "\n",
    "# Generate the all_close_treat data\n",
    "tass5 = tass5[['id', 'companyid', 'mydate', 'min_mydate', 'max_mydate']].drop_duplicates()\n",
    "tass5['closed'] = np.where((tass5['mydate'] == tass5['max_mydate']) & (tass5['max_mydate'] <= 653), 1, 0)\n",
    "tass5['opened'] = np.where((tass5['mydate'] == tass5['min_mydate']) & (tass5['min_mydate'] >= 409), 1, 0)\n",
    "\n",
    "# Collapse data by companyid and mydate\n",
    "collapsed_data = tass5.groupby(['companyid', 'mydate']).sum().reset_index()\n",
    "\n",
    "# Create lagged closed variables\n",
    "for i in range(1, 31):\n",
    "    collapsed_data[f'L{i}closed'] = collapsed_data.groupby('companyid')['closed'].shift(i).fillna(0)\n",
    "\n",
    "# Calculate net openings and closings\n",
    "collapsed_data['net'] = collapsed_data['opened'] - collapsed_data[[f'L{i}closed' for i in range(1, 13)]].sum(axis=1)\n",
    "\n",
    "# Create open_close and all_close_treat variables\n",
    "collapsed_data['open_close'] = np.where((collapsed_data['opened'] == 1) & (collapsed_data['net'] <= 0), 1, 0)\n",
    "collapsed_data['all_close_treat'] = collapsed_data[[f'L{i}closed' for i in range(1, 31)]].max(axis=1)\n",
    "collapsed_data['time_treat_all'] = np.argmax(collapsed_data[[f'L{i}closed' for i in range(1, 31)]].values >= 1, axis=1) + 1\n",
    "collapsed_data['time_treat_all'] = np.where(collapsed_data['all_close_treat'] >= 1, collapsed_data['time_treat_all'], 0)\n",
    "\n",
    "# Keep necessary columns and sort\n",
    "collapsed_data = collapsed_data[['companyid', 'mydate', 'all_close_treat', 'time_treat_all']].sort_values(by=['companyid', 'mydate'])\n",
    "\n",
    "# Save the all_close_treat data\n",
    "collapsed_data.to_csv('all_close_treat.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T03:19:17.951098200Z",
     "start_time": "2024-06-18T03:19:15.066668200Z"
    }
   },
   "id": "3aba75e1e07f76c6",
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "source": [
    "RET_CORR_PAIRS"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "775a2e45520f8dfa"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "MergeError",
     "evalue": "Passing 'suffixes' which cause duplicate columns {'simulcounter_x'} is not allowed.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMergeError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[29], line 14\u001B[0m\n\u001B[0;32m     12\u001B[0m simulcounter_df\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msimulcounter.csv\u001B[39m\u001B[38;5;124m'\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# Step 2: Count the number of funds per firm and filter firms with at least two funds simultaneously\u001B[39;00m\n\u001B[1;32m---> 14\u001B[0m potential_divcorr \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mmerge(potential_divcorr, simulcounter_df, on\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcompanyid\u001B[39m\u001B[38;5;124m'\u001B[39m, how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minner\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     15\u001B[0m potential_divcorr[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msimulcounter\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m potential_divcorr[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msimulcounter_y\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     16\u001B[0m potential_divcorr \u001B[38;5;241m=\u001B[39m potential_divcorr[potential_divcorr[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msimulcounter\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:183\u001B[0m, in \u001B[0;36mmerge\u001B[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001B[0m\n\u001B[0;32m    168\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    169\u001B[0m     op \u001B[38;5;241m=\u001B[39m _MergeOperation(\n\u001B[0;32m    170\u001B[0m         left_df,\n\u001B[0;32m    171\u001B[0m         right_df,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    181\u001B[0m         validate\u001B[38;5;241m=\u001B[39mvalidate,\n\u001B[0;32m    182\u001B[0m     )\n\u001B[1;32m--> 183\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m op\u001B[38;5;241m.\u001B[39mget_result(copy\u001B[38;5;241m=\u001B[39mcopy)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:885\u001B[0m, in \u001B[0;36m_MergeOperation.get_result\u001B[1;34m(self, copy)\u001B[0m\n\u001B[0;32m    881\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mleft, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mright \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_indicator_pre_merge(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mleft, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mright)\n\u001B[0;32m    883\u001B[0m join_index, left_indexer, right_indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_join_info()\n\u001B[1;32m--> 885\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reindex_and_concat(\n\u001B[0;32m    886\u001B[0m     join_index, left_indexer, right_indexer, copy\u001B[38;5;241m=\u001B[39mcopy\n\u001B[0;32m    887\u001B[0m )\n\u001B[0;32m    888\u001B[0m result \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_merge_type)\n\u001B[0;32m    890\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindicator:\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:837\u001B[0m, in \u001B[0;36m_MergeOperation._reindex_and_concat\u001B[1;34m(self, join_index, left_indexer, right_indexer, copy)\u001B[0m\n\u001B[0;32m    834\u001B[0m left \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mleft[:]\n\u001B[0;32m    835\u001B[0m right \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mright[:]\n\u001B[1;32m--> 837\u001B[0m llabels, rlabels \u001B[38;5;241m=\u001B[39m _items_overlap_with_suffix(\n\u001B[0;32m    838\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mleft\u001B[38;5;241m.\u001B[39m_info_axis, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mright\u001B[38;5;241m.\u001B[39m_info_axis, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msuffixes\n\u001B[0;32m    839\u001B[0m )\n\u001B[0;32m    841\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m left_indexer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_range_indexer(left_indexer, \u001B[38;5;28mlen\u001B[39m(left)):\n\u001B[0;32m    842\u001B[0m     \u001B[38;5;66;03m# Pinning the index here (and in the right code just below) is not\u001B[39;00m\n\u001B[0;32m    843\u001B[0m     \u001B[38;5;66;03m#  necessary, but makes the `.take` more performant if we have e.g.\u001B[39;00m\n\u001B[0;32m    844\u001B[0m     \u001B[38;5;66;03m#  a MultiIndex for left.index.\u001B[39;00m\n\u001B[0;32m    845\u001B[0m     lmgr \u001B[38;5;241m=\u001B[39m left\u001B[38;5;241m.\u001B[39m_mgr\u001B[38;5;241m.\u001B[39mreindex_indexer(\n\u001B[0;32m    846\u001B[0m         join_index,\n\u001B[0;32m    847\u001B[0m         left_indexer,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    852\u001B[0m         use_na_proxy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    853\u001B[0m     )\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:2697\u001B[0m, in \u001B[0;36m_items_overlap_with_suffix\u001B[1;34m(left, right, suffixes)\u001B[0m\n\u001B[0;32m   2695\u001B[0m     dups\u001B[38;5;241m.\u001B[39mextend(rlabels[(rlabels\u001B[38;5;241m.\u001B[39mduplicated()) \u001B[38;5;241m&\u001B[39m (\u001B[38;5;241m~\u001B[39mright\u001B[38;5;241m.\u001B[39mduplicated())]\u001B[38;5;241m.\u001B[39mtolist())\n\u001B[0;32m   2696\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dups:\n\u001B[1;32m-> 2697\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MergeError(\n\u001B[0;32m   2698\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPassing \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msuffixes\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m which cause duplicate columns \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mset\u001B[39m(dups)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2699\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnot allowed.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   2700\u001B[0m     )\n\u001B[0;32m   2702\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m llabels, rlabels\n",
      "\u001B[1;31mMergeError\u001B[0m: Passing 'suffixes' which cause duplicate columns {'simulcounter_x'} is not allowed."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the potential_divcorr dataset\n",
    "potential_divcorr = pd.read_csv('potential_divcorr.csv')\n",
    "\n",
    "# Step 1: Count the number of funds per firm at any given point in time (simulcounter)\n",
    "potential_divcorr['simulcounter'] = potential_divcorr.groupby(['companyid', 'mydate']).cumcount() + 1\n",
    "potential_divcorr['maxsimulcounter'] = potential_divcorr.groupby('companyid')['simulcounter'].transform('max')\n",
    "simulcounter_df = potential_divcorr.drop_duplicates(subset=['companyid']).copy()\n",
    "simulcounter_df = simulcounter_df[['companyid', 'maxsimulcounter']].rename(columns={'maxsimulcounter': 'simulcounter'})\n",
    "# Save simulcounter data\n",
    "simulcounter_df.to_csv('simulcounter.csv', index=False)\n",
    "# Step 2: Count the number of funds per firm and filter firms with at least two funds simultaneously\n",
    "potential_divcorr = pd.merge(potential_divcorr, simulcounter_df, on='companyid', how='inner')\n",
    "potential_divcorr['simulcounter'] = potential_divcorr['simulcounter_y']\n",
    "potential_divcorr = potential_divcorr[potential_divcorr['simulcounter'] > 1]\n",
    "\n",
    "# Step 3: Create a sequential fund identifier within the firm (fundcounter)\n",
    "fundcounter_df = potential_divcorr.groupby('id').agg({'companyid': 'max', 'mydate': 'min'}).reset_index()\n",
    "fundcounter_df = fundcounter_df.sort_values(by=['companyid', 'mydate', 'id'])\n",
    "fundcounter_df['fund_counter'] = fundcounter_df.groupby('companyid').cumcount() + 1\n",
    "fundcounter_df = fundcounter_df[['companyid', 'id', 'fund_counter']]\n",
    "\n",
    "# Save fundcounter data\n",
    "fundcounter_df.to_csv('fundcounter.csv', index=False)\n",
    "\n",
    "# Step 4: Create a database of error terms from the 7-factor regressions\n",
    "potential_divcorr['epsilon'] = potential_divcorr['excess_ret'] - potential_divcorr['alpha']\n",
    "epsilon_df = potential_divcorr[['id', 'mydate', 'epsilon']]\n",
    "\n",
    "# Save epsilon data\n",
    "epsilon_df.to_csv('epsilon.csv', index=False)\n",
    "potential_divcorr.to_csv('potential_divcorr.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T11:28:45.299587800Z",
     "start_time": "2024-06-18T11:28:39.706377200Z"
    }
   },
   "id": "f752094e59adfb00",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6dba2ba88b23744c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "RET_CORR_BIG"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d181b256883a4d83"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "import glob\n",
    "import logging\n",
    "\n",
    "# Initialize the logging functionality\n",
    "logging.basicConfig(filename='ret_corr_big.log', level=logging.INFO)\n",
    "\n",
    "# Load datasets\n",
    "potential_divcorr = pd.read_csv('potential_divcorr.csv')\n",
    "epsilon = pd.read_csv('epsilon.csv')\n",
    "simulcounter = pd.read_csv('simulcounter.csv')\n",
    "fundcounter = pd.read_csv('fundcounter.csv')\n",
    "\n",
    "# Filter the data and merge necessary information\n",
    "potential_divcorr = potential_divcorr[['companyid', 'mydate', 'id', 'ret', 'excess_ret', 'alpha']]\n",
    "potential_divcorr = potential_divcorr.sort_values(by=['id', 'mydate'])\n",
    "potential_divcorr = potential_divcorr.merge(epsilon, on=['id', 'mydate'], how='left')\n",
    "potential_divcorr = potential_divcorr.merge(simulcounter, on='companyid', how='inner')\n",
    "\n",
    "# Filter firms with at least two funds simultaneously\n",
    "potential_divcorr = potential_divcorr[potential_divcorr['simulcounter'] > 1]\n",
    "\n",
    "# Pivot data to wide format using actual IDs\n",
    "df_wide = potential_divcorr.pivot_table(index=[\"companyid\", \"mydate\"], columns=\"id\", values=[\"ret\", \"epsilon\"], aggfunc='first')\n",
    "\n",
    "# Flatten multi-index columns\n",
    "df_wide.columns = [f\"{col[0]}_{col[1]}\" for col in df_wide.columns]\n",
    "df_wide.reset_index(inplace=True)\n",
    "\n",
    "# Initialize the parameter `last`\n",
    "last = 65\n",
    "\n",
    "# Function to calculate and save correlations using actual IDs\n",
    "def calculate_and_save_correlations(df_wide, last):\n",
    "    results = []\n",
    "    columns = df_wide.columns\n",
    "    fund_ids = [col.split(\"_\")[1] for col in columns if \"ret_\" in col]\n",
    "    unique_fund_ids = sorted(set(fund_ids))\n",
    "    \n",
    "    for idx1 in range(len(unique_fund_ids)):\n",
    "        fund_id1 = unique_fund_ids[idx1]\n",
    "        for idx2 in range(idx1 + 1, len(unique_fund_ids)):\n",
    "            if idx2 - idx1 >= last:\n",
    "                break\n",
    "            fund_id2 = unique_fund_ids[idx2]\n",
    "            relevant_columns = [f\"ret_{fund_id1}\", f\"ret_{fund_id2}\", f\"epsilon_{fund_id1}\", f\"epsilon_{fund_id2}\"]\n",
    "            if all(col in df_wide.columns for col in relevant_columns):\n",
    "                temp_df = df_wide[['companyid', 'mydate'] + relevant_columns].dropna()\n",
    "                if len(temp_df) >= 12:\n",
    "                    correlations = temp_df.corr()\n",
    "                    ret_corr = correlations.loc[f\"ret_{fund_id1}\", f\"ret_{fund_id2}\"]\n",
    "                    epsilon_corr = correlations.loc[f\"epsilon_{fund_id1}\", f\"epsilon_{fund_id2}\"]\n",
    "                    results.append({\n",
    "                        \"companyid\": temp_df['companyid'].iloc[0],\n",
    "                        \"div_cons1\": fund_id1,\n",
    "                        \"div_cons2\": fund_id2,\n",
    "                        \"div_corr\": ret_corr,\n",
    "                        \"corr_eps\": epsilon_corr\n",
    "                    })\n",
    "                    # Save results to CSV, one file per pair\n",
    "                    result_df = pd.DataFrame([results[-1]])\n",
    "                    result_df.to_csv(f\"div_corr_{fund_id1}_{fund_id2}.csv\", index=False)\n",
    "\n",
    "# Calculate and save correlations\n",
    "calculate_and_save_correlations(df_wide, last)\n",
    "\n",
    "# Combine all correlation files into a single DataFrame\n",
    "complete_divcorr = pd.DataFrame()\n",
    "correlation_files = glob.glob(\"div_corr_*.csv\")\n",
    "for file in correlation_files:\n",
    "    temp_df = pd.read_csv(file)\n",
    "    complete_divcorr = pd.concat([complete_divcorr, temp_df], ignore_index=True)\n",
    "    complete_divcorr.sort_values(by=['companyid', 'div_cons1', 'div_cons2'], inplace=True)\n",
    "    complete_divcorr['div_corr_id'] = range(1, len(complete_divcorr)+1)\n",
    "complete_divcorr.to_csv(\"complete_divcorr.csv\", index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T16:04:44.722634600Z",
     "start_time": "2024-06-18T16:04:27.370623500Z"
    }
   },
   "id": "5ea412d96e56ea00",
   "execution_count": 93
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[80], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m n \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m----> 4\u001B[0m     os\u001B[38;5;241m.\u001B[39mremove(glob(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdiv_corr_*.csv\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m:\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "for x in range(1, 100000):\n",
    "    n = x + 1\n",
    "    try:\n",
    "        os.remove(glob('div_corr_*.csv'))\n",
    "    except FileNotFoundError:\n",
    "        continue"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T14:57:25.479603500Z",
     "start_time": "2024-06-18T14:57:25.421471400Z"
    }
   },
   "id": "2391a814e511c740",
   "execution_count": 80
  },
  {
   "cell_type": "markdown",
   "source": [
    "RET_CORR_INTEGRATE -> zit al bij BIG "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbe2bcdbed64671c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Enig hier is dat ipv tm 65, de laatste 65 worden gepakt, nog niet helemaal duidelijk wat het uitmaakt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "231fb5954cc552fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "RET_CORR_INTEGRATE 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c53a99283643eb27"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load necessary datasets\n",
    "complete_divcorr = pd.read_csv('complete_divcorr.csv')\n",
    "potential_treat0 = pd.read_csv('potential_treat0.csv')\n",
    "\n",
    "# Step 1: Make a list of every fund (id) in complete_divcorr and call it potential_treat1\n",
    "divcorr_id1 = complete_divcorr[['div_cons1']].rename(columns={'div_cons1': 'id'})\n",
    "divcorr_id2 = complete_divcorr[['div_cons2']].rename(columns={'div_cons2': 'id'})\n",
    "potential_treat1 = pd.concat([divcorr_id1, divcorr_id2]).drop_duplicates().sort_values(by='id')\n",
    "potential_treat1.to_csv('potential_treat1.csv', index=False)\n",
    "\n",
    "# Step 2: Merge in everything from potential_treat0\n",
    "potential_treat2 = potential_treat0.merge(potential_treat1, on='id', how='left')\n",
    "potential_treat2['closedxcrisis'].fillna(0, inplace=True)\n",
    "potential_treat2['closedxcrisis'] = potential_treat2['closedxcrisis'].astype(int)\n",
    "potential_treat2['fund_closedxcrisis'] = potential_treat2.groupby('id')['closedxcrisis'].transform('max')\n",
    "potential_treat2.to_csv('potential_treat2.csv', index=False)\n",
    "\n",
    "# Step 3: Create a file of ids with min_mydate\n",
    "minmydate = potential_treat2[potential_treat2['mydate'] == potential_treat2['min_mydate']]\n",
    "minmydate = minmydate[['id', 'min_mydate']]\n",
    "minmydate.to_csv('minmydate.csv', index=False)\n",
    "\n",
    "oth_minmydate = minmydate.rename(columns={'id': 'other_fund', 'min_mydate': 'oth_minmydate'})\n",
    "oth_minmydate.to_csv('oth_minmydate.csv', index=False)\n",
    "\n",
    "# Step 4: Eliminate \"twins\"\n",
    "# Batch 1\n",
    "batch1 = complete_divcorr[['companyid', 'div_cons1', 'div_cons2', 'div_corr', 'corr_eps']]\n",
    "batch1 = batch1.rename(columns={'div_cons1': 'id', 'div_cons2': 'other_fund'})\n",
    "batch1.to_csv('batch1.csv', index=False)\n",
    "\n",
    "# Batch 2\n",
    "batch2 = complete_divcorr[['companyid', 'div_cons1', 'div_cons2', 'div_corr', 'corr_eps']]\n",
    "batch2 = batch2.rename(columns={'div_cons2': 'id', 'div_cons1': 'other_fund'})\n",
    "batch2 = pd.concat([batch2, batch1]).drop_duplicates().sort_values(by=['companyid', 'id', 'other_fund'])\n",
    "\n",
    "batch2 = batch2.merge(minmydate, on='id', how='inner').rename(columns={'min_mydate': 'id_minmydate'})\n",
    "batch2 = batch2.merge(oth_minmydate, on='other_fund', how='inner')\n",
    "batch2.to_csv('batch2.csv', index=False)\n",
    "\n",
    "# Batch 3: Focus on (unique) funds with div_corr > 0.985\n",
    "batch3 = batch2.copy()\n",
    "batch3['corr_eps'].replace(-999, np.nan, inplace=True)\n",
    "batch3 = batch3[batch3['div_corr'] > 0.985]\n",
    "batch3.drop_duplicates(subset=['id', 'other_fund'], inplace=True)\n",
    "\n",
    "batch3['keep_id'] = np.where(batch3['id_minmydate'] <= batch3['oth_minmydate'], 1, 0)\n",
    "batch3['keep_other_fund'] = np.where(batch3['oth_minmydate'] < batch3['id_minmydate'], 1, 0)\n",
    "batch3.to_csv('batch3.csv', index=False)\n",
    "\n",
    "# Batch 4: Identify other_fund ids when keep_id==0 for an id and rename other_fund id\n",
    "batch4 = batch3[batch3['keep_id'] == 0][['other_fund', 'div_corr']]\n",
    "batch4 = batch4.rename(columns={'other_fund': 'id'})\n",
    "batch4.to_csv('batch4.csv', index=False)\n",
    "\n",
    "# Highcorr_set: Set of funds to be kept, even though they are highly correlated\n",
    "highcorr_set = pd.concat([\n",
    "    batch3[batch3['keep_other_fund'] == 0][['id', 'div_corr']],\n",
    "    batch4\n",
    "]).drop_duplicates(subset=['id']).sort_values(by='id')\n",
    "highcorr_set.to_csv('highcorr_set.csv', index=False)\n",
    "\n",
    "# Dropcorr_set: Set of \"twin\" funds to be dropped\n",
    "dropcorr_set = batch3[['id']].drop_duplicates().sort_values(by='id')\n",
    "dropcorr_set = dropcorr_set.merge(highcorr_set, on='id', how='left', indicator=True)\n",
    "dropcorr_set = dropcorr_set[dropcorr_set['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "dropcorr_set.to_csv('dropcorr_set.csv', index=False)\n",
    "\n",
    "# Define the test set\n",
    "intheset = potential_treat2.copy()\n",
    "intheset = intheset.merge(dropcorr_set, on='id', how='left', indicator=True)\n",
    "intheset['intheset'] = 1\n",
    "intheset.loc[intheset['crisis'] == 1, 'intheset'] = 0\n",
    "intheset.loc[intheset['fund_closedxcrisis'] == 1, 'intheset'] = 0\n",
    "intheset.loc[intheset['min_mydate'] > 562, 'intheset'] = 0\n",
    "intheset.loc[intheset['_merge'] == 'both', 'intheset'] = 0\n",
    "intheset.drop(columns=['_merge'], inplace=True)\n",
    "intheset = intheset[intheset['intheset'] == 1]\n",
    "\n",
    "intheset['intheset2'] = np.where((intheset['mydate'] <= 625) & (intheset['mydate'] >= 617), 0, intheset['intheset'])\n",
    "intheset['counter'] = intheset.groupby('id').cumcount() + 1\n",
    "intheset['maxcounter'] = intheset.groupby('id')['counter'].transform('max')\n",
    "intheset['intheset'] = np.where(intheset['maxcounter'] < 12, 0, intheset['intheset'])\n",
    "\n",
    "intheset = intheset[intheset['intheset'] == 1][['id', 'mydate', 'intheset', 'intheset2']]\n",
    "intheset.to_csv('intheset.csv', index=False)\n",
    "\n",
    "# Bring in all pairwise correlations\n",
    "integrate2 = complete_divcorr[['companyid', 'div_cons1', 'div_cons2', 'div_corr', 'corr_eps']]\n",
    "integrate2 = integrate2.rename(columns={'div_cons2': 'id', 'div_cons1': 'other_fund'})\n",
    "integrate2 = pd.concat([integrate2, batch1]).drop_duplicates().sort_values(by=['companyid', 'id', 'other_fund'])\n",
    "integrate2['corr_eps'].replace(-999, np.nan, inplace=True)\n",
    "integrate2.to_csv('integrate2.csv', index=False)\n",
    "\n",
    "# Identify funds with div_corr that were closed during the crisis\n",
    "closed = potential_treat2[potential_treat2['closedxcrisis'] == 1][['id']]\n",
    "closed.to_csv('closed.csv', index=False)\n",
    "\n",
    "# Merge closed fund with list of funds w/div_corr to find the \"real\" treatments\n",
    "integrate3 = closed.merge(integrate2, on='id', how='inner')\n",
    "integrate3['treatment99'] = np.where(integrate3['div_corr'] < 0.985, 1, 0)\n",
    "integrate3['treatment90'] = np.where(integrate3['div_corr'] < 0.895, 1, 0)\n",
    "integrate3['treatment99_eps'] = np.where((integrate3['corr_eps'] < 0.985) & (integrate3['corr_eps'].notna()), 1, 0)\n",
    "integrate3['treatment90_eps'] = np.where((integrate3['corr_eps'] < 0.895) & (integrate3['corr_eps'].notna()), 1, 0)\n",
    "\n",
    "integrate3 = integrate3.rename(columns={'id': 'treated_id', 'other_fund': 'id'}).sort_values(by=['companyid', 'id'])\n",
    "integrate3.to_csv('integrate3.csv', index=False)\n",
    "\n",
    "# Merge with potential_treat2\n",
    "integrate3 = integrate3.merge(potential_treat2, on=['companyid', 'id'], how='outer')\n",
    "\n",
    "# DIV_CORR < 0.985\n",
    "integrate3['closedxcrisis2'] = integrate3['closedxcrisis']\n",
    "integrate3['closedxcrisis3'] = integrate3['closedxcrisis']\n",
    "integrate3['closedxcrisis4'] = integrate3['closedxcrisis']\n",
    "integrate3.loc[integrate3['div_corr'] >= 0.985, 'closedxcrisis'] = 0\n",
    "integrate3['fund_closedxcrisis'] = integrate3.groupby('id')['closedxcrisis'].transform('max')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T15:23:21.950948400Z",
     "start_time": "2024-06-18T15:23:07.126614Z"
    }
   },
   "id": "3c9f278779252fd1",
   "execution_count": 90
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integration completed and temporary files cleaned up.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Instantaneous and constant FIRM measures of closure during the crisis\n",
    "integrate3['firm_closedxcrisis'] = integrate3.groupby(['companyid', 'mydate'])['closedxcrisis'].transform('max')\n",
    "integrate3['max_firm_closedxcrisis'] = integrate3.groupby('companyid')['closedxcrisis'].transform('max')\n",
    "\n",
    "# A fund cannot be treated if it started within one year of the financial crisis\n",
    "integrate3.loc[integrate3['min_mydate'] > 562, 'max_firm_closedxcrisis'] = 0\n",
    "integrate3.loc[integrate3['div_corr'] >= 0.985, 'max_firm_closedxcrisis'] = 0\n",
    "\n",
    "# crisis_treat24 equals one for two years after the end of the crisis\n",
    "integrate3['crisis_treat24'] = 0\n",
    "integrate3.loc[(integrate3['max_firm_closedxcrisis'] == 1) & (integrate3['mydate'] >= 595) & (integrate3['mydate'] <= 619), 'crisis_treat24'] = 1\n",
    "\n",
    "# DIV_CORR < 0.895\n",
    "integrate3.loc[integrate3['div_corr'] >= 0.895, 'closedxcrisis2'] = 0\n",
    "integrate3['fund_closedxcrisis2'] = integrate3.groupby('id')['closedxcrisis2'].transform('max')\n",
    "\n",
    "# Instantaneous and constant FIRM measures of closure during the crisis\n",
    "integrate3['firm_closedxcrisis2'] = integrate3.groupby(['companyid', 'mydate'])['closedxcrisis2'].transform('max')\n",
    "integrate3['max_firm_closedxcrisis2'] = integrate3.groupby('companyid')['closedxcrisis2'].transform('max')\n",
    "\n",
    "# A fund cannot be treated if it started within one year of the financial crisis\n",
    "integrate3.loc[integrate3['min_mydate'] > 562, 'max_firm_closedxcrisis2'] = 0\n",
    "integrate3.loc[integrate3['div_corr'] >= 0.895, 'max_firm_closedxcrisis2'] = 0\n",
    "\n",
    "# crisis_treat24 equals one for two years after the end of the crisis\n",
    "integrate3['crisis_treat24_90'] = 0\n",
    "integrate3.loc[(integrate3['max_firm_closedxcrisis2'] == 1) & (integrate3['mydate'] >= 595) & (integrate3['mydate'] <= 619), 'crisis_treat24_90'] = 1\n",
    "\n",
    "# CORR_EPS < 0.985\n",
    "integrate3.loc[integrate3['corr_eps'] >= 0.985, 'closedxcrisis3'] = 0\n",
    "integrate3['fund_closedxcrisis3'] = integrate3.groupby('id')['closedxcrisis3'].transform('max')\n",
    "\n",
    "# Instantaneous and constant FIRM measures of closure during the crisis\n",
    "integrate3['firm_closedxcrisis3'] = integrate3.groupby(['companyid', 'mydate'])['closedxcrisis3'].transform('max')\n",
    "integrate3['max_firm_closedxcrisis3'] = integrate3.groupby('companyid')['closedxcrisis3'].transform('max')\n",
    "\n",
    "# A fund cannot be treated if it started within one year of the financial crisis\n",
    "integrate3.loc[integrate3['min_mydate'] > 562, 'max_firm_closedxcrisis3'] = 0\n",
    "integrate3.loc[integrate3['div_corr'] >= 0.985, 'max_firm_closedxcrisis3'] = 0\n",
    "\n",
    "# crisis_treat24 equals one for two years after the end of the crisis\n",
    "integrate3['crisis_treat24_99eps'] = 0\n",
    "integrate3.loc[(integrate3['max_firm_closedxcrisis3'] == 1) & (integrate3['mydate'] >= 595) & (integrate3['mydate'] <= 619), 'crisis_treat24_99eps'] = 1\n",
    "\n",
    "# CORR_EPS < 0.895\n",
    "integrate3.loc[integrate3['corr_eps'] >= 0.895, 'closedxcrisis4'] = 0\n",
    "integrate3['fund_closedxcrisis4'] = integrate3.groupby('id')['closedxcrisis4'].transform('max')\n",
    "\n",
    "# Instantaneous and constant FIRM measures of closure during the crisis\n",
    "integrate3['firm_closedxcrisis4'] = integrate3.groupby(['companyid', 'mydate'])['closedxcrisis4'].transform('max')\n",
    "integrate3['max_firm_closedxcrisis4'] = integrate3.groupby('companyid')['closedxcrisis4'].transform('max')\n",
    "\n",
    "# A fund cannot be treated if it started within one year of the financial crisis\n",
    "integrate3.loc[integrate3['min_mydate'] > 562, 'max_firm_closedxcrisis4'] = 0\n",
    "integrate3.loc[integrate3['div_corr'] >= 0.895, 'max_firm_closedxcrisis4'] = 0\n",
    "\n",
    "# crisis_treat24 equals one for two years after the end of the crisis\n",
    "integrate3['crisis_treat24_90eps'] = 0\n",
    "integrate3.loc[(integrate3['max_firm_closedxcrisis4'] == 1) & (integrate3['mydate'] >= 595) & (integrate3['mydate'] <= 619), 'crisis_treat24_90eps'] = 1\n",
    "\n",
    "integrate3.to_csv('integrate4.csv', index=False)\n",
    "\n",
    "# Clean up\n",
    "import os\n",
    "files_to_erase = [\n",
    "    'batch1.csv', 'batch3.csv', 'batch4.csv', 'potential_treat1.csv', \n",
    "    'integrate2.csv', 'highcorr_set.csv'\n",
    "]\n",
    "\n",
    "for file in files_to_erase:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "\n",
    "print(\"Integration completed and temporary files cleaned up.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-18T15:26:31.669642Z",
     "start_time": "2024-06-18T15:26:16.406956400Z"
    }
   },
   "id": "e5771ae3a8376d14",
   "execution_count": 92
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "515bac032aacf35"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
